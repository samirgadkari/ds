{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10\n",
    "  - 10 categories of 32 x 32 sized color images\n",
    "  - 50000 training and 10000 testing samples\n",
    "  \n",
    "The full CIFAR dataset contains 80 million tiny colored images.\n",
    "  - The main page: https://www.cs.toronto.edu/%7Ekriz/cifar.html\n",
    "  - About CIFAR: https://www.cs.toronto.edu/%7Ekriz/learning-features-2009-TR.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import tensorflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from tensorflow.keras.datasets.cifar10 import load_data\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, BatchNormalization, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can download the data from the original link above and load it like this ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "file_dicts = {}\n",
    "for i in range(1, 6):\n",
    "    batch = f'data_batch_{i}'\n",
    "    filename = os.path.join('.', 'data', 'cifar', 'cifar-10-batches-py', batch)\n",
    "    file_dicts[i-1] = unpickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_data(data, type_):\n",
    "    a = data[0][type_]\n",
    "    for i in range(1, 5):\n",
    "        a = np.r_[a, data[i][type_]]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = append_data(file_dicts, b'data')\n",
    "y_full = append_data(file_dicts, b'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3072), (50000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full.shape, y_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = os.path.join('.', 'data', 'cifar', 'cifar-10-batches-py', 'test_batch')\n",
    "test_file_dict = unpickle(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_file_dict[b'data']\n",
    "y_test = test_file_dict[b'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StratifiedShuffleSplit to split training data into training and validation. \n",
    "# This will ensure that the training and validation data has an equal proportion of classes.\n",
    "#\n",
    "split = StratifiedShuffleSplit(n_splits=1, train_size=0.8, test_size=0.2) # We don't need to specify both test/train.\n",
    "                                                                          # sizes, but it is good for clarity.\n",
    "for train_idx, test_idx in split.split(X_full, y_full):\n",
    "    X_train, X_val = X_full[train_idx], X_full[test_idx]\n",
    "    y_train, y_val = y_full[train_idx], y_full[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 3072), 40000, (10000, 3072), 10000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, len(y_train), X_test.shape, len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9    0.1\n",
       " 8    0.1\n",
       " 7    0.1\n",
       " 6    0.1\n",
       " 5    0.1\n",
       " 4    0.1\n",
       " 3    0.1\n",
       " 2    0.1\n",
       " 1    0.1\n",
       " 0    0.1\n",
       " dtype: float64, 7    0.1\n",
       " 6    0.1\n",
       " 5    0.1\n",
       " 4    0.1\n",
       " 3    0.1\n",
       " 2    0.1\n",
       " 9    0.1\n",
       " 1    0.1\n",
       " 8    0.1\n",
       " 0    0.1\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate that the split shows the correct proportion of classes\n",
    "pd.Series(y_train).value_counts(normalize=True), pd.Series(y_val).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... or an easier way is to use Tensorflow's load_data() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3072), (50000,), (10000, 3072), (10000,))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9    0.1\n",
       " 8    0.1\n",
       " 7    0.1\n",
       " 6    0.1\n",
       " 5    0.1\n",
       " 4    0.1\n",
       " 3    0.1\n",
       " 2    0.1\n",
       " 1    0.1\n",
       " 0    0.1\n",
       " dtype: float64, 7    0.1\n",
       " 6    0.1\n",
       " 5    0.1\n",
       " 4    0.1\n",
       " 3    0.1\n",
       " 2    0.1\n",
       " 9    0.1\n",
       " 1    0.1\n",
       " 8    0.1\n",
       " 0    0.1\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate that the split shows the correct proportion of classes\n",
    "pd.Series(y_train).value_counts(normalize=True), pd.Series(y_test).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StratifiedShuffleSplit to split training data into training and validation. \n",
    "# This will ensure that the training and validation data has an equal proportion of classes.\n",
    "#\n",
    "split = StratifiedShuffleSplit(n_splits=1, train_size=0.8, test_size=0.2) # We don't need to specify both test/train.\n",
    "                                                                          # sizes, but it is good for clarity.\n",
    "for train_idx, test_idx in split.split(X_train, y_train):\n",
    "    X_train_1, X_val = X_train[train_idx], X_train[test_idx]\n",
    "    y_train_1, y_val = y_train[train_idx], y_train[test_idx]\n",
    "\n",
    "X_train = X_train_1\n",
    "y_train = y_train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 3072), (40000,), (10000, 3072), (10000,))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model with Batch Normalization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(with_bn=False):\n",
    "    model = Sequential([\n",
    "        InputLayer(input_shape=[3072])\n",
    "    ])\n",
    "\n",
    "    if with_bn:\n",
    "        model.add(BatchNormalization())                           # Add BN layer after input\n",
    "\n",
    "    HeNormalDense = partial(Dense,                                # Put all your common init here.\n",
    "                            kernel_initializer='he_normal',\n",
    "                            use_bias=False if with_bn else True)  # BN has bias, so remove it\n",
    "                                                                  # from the Dense layer.\n",
    "\n",
    "    for _ in range(20):\n",
    "        model.add(HeNormalDense(100))\n",
    "        \n",
    "        if with_bn:\n",
    "            model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Activation('elu'))\n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))                   # Output layer\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 500,210\n",
      "Trainable params: 500,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(with_bn=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='nadam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = EarlyStopping(patience=10,\n",
    "                                  restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 12s 288us/sample - loss: 1.9412 - accuracy: 0.2660 - val_loss: 1.8840 - val_accuracy: 0.2890\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 11s 278us/sample - loss: 1.8447 - accuracy: 0.3087 - val_loss: 1.8206 - val_accuracy: 0.3314\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 11s 280us/sample - loss: 1.8050 - accuracy: 0.3279 - val_loss: 1.8186 - val_accuracy: 0.3293\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 11s 278us/sample - loss: 1.7873 - accuracy: 0.3383 - val_loss: 1.7932 - val_accuracy: 0.3478\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 11s 278us/sample - loss: 1.7822 - accuracy: 0.3439 - val_loss: 1.8279 - val_accuracy: 0.3370\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 12s 288us/sample - loss: 1.8248 - accuracy: 0.3296 - val_loss: 1.9413 - val_accuracy: 0.2848\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 12s 304us/sample - loss: 1.8626 - accuracy: 0.3108 - val_loss: 1.8292 - val_accuracy: 0.3297\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 12s 299us/sample - loss: 1.8202 - accuracy: 0.3307 - val_loss: 1.8832 - val_accuracy: 0.3244\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 12s 292us/sample - loss: 1.7921 - accuracy: 0.3444 - val_loss: 1.7910 - val_accuracy: 0.3555\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 11s 279us/sample - loss: 1.7712 - accuracy: 0.3504 - val_loss: 1.7696 - val_accuracy: 0.3611\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 11s 278us/sample - loss: 1.7599 - accuracy: 0.3549 - val_loss: 1.8278 - val_accuracy: 0.3304\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 11s 280us/sample - loss: 1.7890 - accuracy: 0.3452 - val_loss: 1.7729 - val_accuracy: 0.3537\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 11s 279us/sample - loss: 1.7465 - accuracy: 0.3609 - val_loss: 1.7871 - val_accuracy: 0.3536\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 11s 281us/sample - loss: 1.7823 - accuracy: 0.3447 - val_loss: 1.7855 - val_accuracy: 0.3550\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 11s 281us/sample - loss: 1.7370 - accuracy: 0.3628 - val_loss: 1.7391 - val_accuracy: 0.3654\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 11s 280us/sample - loss: 1.7156 - accuracy: 0.3735 - val_loss: 1.7253 - val_accuracy: 0.3782\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 11s 282us/sample - loss: 1.7101 - accuracy: 0.3755 - val_loss: 1.7256 - val_accuracy: 0.3733\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 11s 284us/sample - loss: 1.7071 - accuracy: 0.3801 - val_loss: 1.7720 - val_accuracy: 0.3599\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 11s 285us/sample - loss: 1.6975 - accuracy: 0.3822 - val_loss: 1.7244 - val_accuracy: 0.3772\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 11s 283us/sample - loss: 1.6890 - accuracy: 0.3840 - val_loss: 1.7161 - val_accuracy: 0.3715\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 11s 287us/sample - loss: 1.6837 - accuracy: 0.3839 - val_loss: 1.7140 - val_accuracy: 0.3822\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 11s 283us/sample - loss: 2.3240 - accuracy: 0.2919 - val_loss: 1.8959 - val_accuracy: 0.2619\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 11s 285us/sample - loss: 1.8541 - accuracy: 0.2795 - val_loss: 1.8638 - val_accuracy: 0.2861\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 12s 288us/sample - loss: 1.8327 - accuracy: 0.2913 - val_loss: 1.8765 - val_accuracy: 0.2896\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 11s 286us/sample - loss: 1.8157 - accuracy: 0.2999 - val_loss: 1.8782 - val_accuracy: 0.2908\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 11s 287us/sample - loss: 1.8085 - accuracy: 0.3075 - val_loss: 1.8319 - val_accuracy: 0.3104\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 11s 287us/sample - loss: 1.8011 - accuracy: 0.3111 - val_loss: 1.8371 - val_accuracy: 0.2932\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 11s 284us/sample - loss: 1.7974 - accuracy: 0.3133 - val_loss: 1.8087 - val_accuracy: 0.3119\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 11s 286us/sample - loss: 1.8019 - accuracy: 0.3185 - val_loss: 1.8943 - val_accuracy: 0.2833\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 11s 287us/sample - loss: 1.7998 - accuracy: 0.3189 - val_loss: 1.8089 - val_accuracy: 0.3204\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 11s 287us/sample - loss: 42.3642 - accuracy: 0.2354 - val_loss: 2.0864 - val_accuracy: 0.2057\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization (BatchNo (None, 3072)              12288     \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 100)               307200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 518,498\n",
      "Trainable params: 508,354\n",
      "Non-trainable params: 10,144\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(with_bn=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='nadam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = EarlyStopping(patience=10,\n",
    "                                  restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 25s 618us/sample - loss: 1.8552 - accuracy: 0.3319 - val_loss: 1.7351 - val_accuracy: 0.3722\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 17s 430us/sample - loss: 1.6971 - accuracy: 0.3950 - val_loss: 1.5981 - val_accuracy: 0.4306\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 17s 429us/sample - loss: 1.6306 - accuracy: 0.4152 - val_loss: 1.5751 - val_accuracy: 0.4474\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 16s 407us/sample - loss: 1.5817 - accuracy: 0.4383 - val_loss: 1.5593 - val_accuracy: 0.4394\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 16s 407us/sample - loss: 1.5344 - accuracy: 0.4584 - val_loss: 1.5144 - val_accuracy: 0.4670\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 17s 435us/sample - loss: 1.4992 - accuracy: 0.4666 - val_loss: 1.5039 - val_accuracy: 0.4695\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 17s 428us/sample - loss: 1.4592 - accuracy: 0.4837 - val_loss: 1.4906 - val_accuracy: 0.4761\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 17s 421us/sample - loss: 1.4335 - accuracy: 0.4944 - val_loss: 1.4514 - val_accuracy: 0.4868\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 17s 421us/sample - loss: 1.4102 - accuracy: 0.5020 - val_loss: 1.4477 - val_accuracy: 0.4911\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 17s 428us/sample - loss: 1.3776 - accuracy: 0.5142 - val_loss: 1.3975 - val_accuracy: 0.5104\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 17s 435us/sample - loss: 1.3586 - accuracy: 0.5204 - val_loss: 1.3836 - val_accuracy: 0.5128\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 17s 432us/sample - loss: 1.3402 - accuracy: 0.5275 - val_loss: 1.3938 - val_accuracy: 0.5143\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 17s 432us/sample - loss: 1.3140 - accuracy: 0.5368 - val_loss: 1.3939 - val_accuracy: 0.5128\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 17s 432us/sample - loss: 1.2930 - accuracy: 0.5424 - val_loss: 1.3959 - val_accuracy: 0.5180\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 17s 435us/sample - loss: 1.2733 - accuracy: 0.5520 - val_loss: 1.4036 - val_accuracy: 0.5189\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 17s 436us/sample - loss: 1.2565 - accuracy: 0.5575 - val_loss: 1.3707 - val_accuracy: 0.5266\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 17s 435us/sample - loss: 1.2474 - accuracy: 0.5609 - val_loss: 1.3553 - val_accuracy: 0.5248\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 17s 437us/sample - loss: 1.2247 - accuracy: 0.5692 - val_loss: 1.3695 - val_accuracy: 0.5258\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 17s 435us/sample - loss: 1.2136 - accuracy: 0.5728 - val_loss: 1.3440 - val_accuracy: 0.5365\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 17s 437us/sample - loss: 1.1955 - accuracy: 0.5790 - val_loss: 1.3621 - val_accuracy: 0.5282\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 18s 440us/sample - loss: 1.1782 - accuracy: 0.5870 - val_loss: 1.3714 - val_accuracy: 0.5256\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 18s 443us/sample - loss: 1.1631 - accuracy: 0.5912 - val_loss: 1.3833 - val_accuracy: 0.5227\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 18s 438us/sample - loss: 1.1463 - accuracy: 0.5949 - val_loss: 1.3631 - val_accuracy: 0.5246\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 18s 440us/sample - loss: 1.1326 - accuracy: 0.6012 - val_loss: 1.3998 - val_accuracy: 0.5212\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 18s 440us/sample - loss: 1.1142 - accuracy: 0.6072 - val_loss: 1.3737 - val_accuracy: 0.5265\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 18s 441us/sample - loss: 1.1110 - accuracy: 0.6105 - val_loss: 1.4055 - val_accuracy: 0.5217\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 18s 442us/sample - loss: 1.0888 - accuracy: 0.6181 - val_loss: 1.3903 - val_accuracy: 0.5308\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 18s 442us/sample - loss: 1.0809 - accuracy: 0.6195 - val_loss: 1.4414 - val_accuracy: 0.5283\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 18s 442us/sample - loss: 1.0743 - accuracy: 0.6238 - val_loss: 1.3767 - val_accuracy: 0.5273\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
