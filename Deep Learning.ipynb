{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A large portion of this code is taken from Aur√©lien G√©ron's: Hands-On machine learning with SciKit-Learn, Keras and Tensorflow (2nd edition). Also code taken from his github repo is included here. I have put comments based on information in the book as well as information I found elsewhere.\n",
    "\n",
    "## Chapter 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding gradients problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanishing gradients:\n",
    "  - If the gradients at the upper layer are small, they're divided and assigned to each neuron in the lower layer.\n",
    "    These values are then further divided and assigned to each neron in the layer below it.\n",
    "    As the number of layers are large in a Deep Neural Network, the gradients at the bottom become zero.\n",
    "    During forward propagation, this causes the weights to remain the same and training never converges.\n",
    "Exploding gradients:\n",
    "  - If the loss function has a steep gradient at the location where you're computing it,\n",
    "    there will be a large change in the weight values. This in turn can cause you to overshoot the minima\n",
    "    and land on the other side of the loss function. If the gradient at that point is also large,\n",
    "    this can cause another large change in the weight values, and another overshoot.\n",
    "  - Sometimes these overshoots will get larger and larger getting you more and more away from the minima.\n",
    "    Sometimes these overshoots will get you into a region of the loss function where the gradients are smaller,\n",
    "    thus temporarily keeping the changes to the weights low. But if you get back down to where the\n",
    "    gradients are large, you may find yourself again overshooting. This will cause you to oscillate,\n",
    "    and never get to the minima.\n",
    "  - More generally, neural networks suffer from unstable gradients. \n",
    "    Different layers may learn at widely different speeds.\n",
    "\n",
    "These problems result from using the Sigmoid activation function, \n",
    "and the Standard Normal initial weight distribution\n",
    "(0 mean, 1 std. dev.). With this initialization, \n",
    "the input variance for each layer increases at the output of that layer. \n",
    "A large variance means there are many large values being multiplied by the weights and added to get the next layer's input.\n",
    "This causes the top layer's sigmoid function to get input that saturates it's output.\n",
    "With a saturated output, the gradient is close to zero.\n",
    "Backpropagation has no gradient to propagate back,\n",
    "any small gradient keeps getting diluted at each lower layer.\n",
    "The Sigmoid function has a mean of 0.5, which exacerbates the problem.\n",
    "The Hyperbolic tangent function behaves better (since it's mean is 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of weights and biases\n",
    "\n",
    "  - If weights are initialized to the same value, then each neuron is equivalent to any other neuron. It's like saying you only have one neuron per layer. Such a network is impossible to converge.\n",
    "  - Biases can be initialized to the same value. Assymetry is acheived due to randomly initialized weights as they are used when calculating the next value of the bias for each neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glorot and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution is to use Glorot initialization:\n",
    "$$fan_{avg} = \\frac{fan_{in} + fan_{out}}{2}$$\n",
    "Initialize weights with Normal distribution with 0 mean, variance $\\sigma^2 = \\frac{1}{fan_{avg}}$\n",
    "\n",
    "Or, a uniform distribution between $\\pm$r, with $r = \\sqrt{\\frac{3}{fan_{avg}}} = \\sqrt{3{\\sigma}^2}$\n",
    "\n",
    "Glorot initialization can speed up training considerably.\n",
    "\n",
    "| Initialization | Activation functions | $\\sigma^2$ (Normal) |\n",
    "|---|---|---|\n",
    "|Glorot|None, tanh, logistic, softmax|$\\frac{1}{fan_{avg}}$|\n",
    "|He|ReLU and variants|$\\frac{2}{fan_{in}}$|\n",
    "|LeCun|SELU|$\\frac{1}{fan_{in}}$|\n",
    "\n",
    "By default Keras uses Glorot initialization with a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use He init with ùëìùëéùëõ_ùëñùëõ:\n",
    "\n",
    "# keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal') \n",
    "# or kernel_initializer='he_uniform'\n",
    "\n",
    "# To use He init with ùëìùëéùëõ_ùëéùë£ùëî:\n",
    "# he_avg_init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg', distribution='uniform')\n",
    "# keras.layers.Dense(10, activation='sigmoid', kernel_initializer=he_avg_init)\n",
    "\n",
    "# With distribution=\"uniform\", samples are drawn from a uniform distribution within [-limit, limit], with\n",
    "#     limit = sqrt(3 * scale / n).\n",
    "#     n = number of input weights\n",
    "# So you can specify the limits of the uniform distribution by setting the scale value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU (Rectified Linear Unit):\n",
    "  - $$\\begin{align}RELU(z) & = 0 \\;\\;\\; if \\; z < 0 \\\\\n",
    "                       & = z \\;\\;\\; if \\; z >= 0 \\end{align}$$                   \n",
    "  - ReLU function thus does not saturate for positive values.\n",
    "  - It is also fast to compute, since we only have to look at\n",
    "    z to decide the output.\n",
    "  - If a layer's weighted sums are negative for all input instances,\n",
    "    the output is 0 (and stays 0 forever). This is the dying ReLU problem.\n",
    "  - Use Leaky ReLU instead\n",
    "  \n",
    "**The problems with ReLU occur since it's derivative is zero sometimes (when z < 0). All the solutions below have non-zero derivatives for any z value.**\n",
    "\n",
    "Leaky ReLU:\n",
    "  - $$Leaky \\; ReLU_\\alpha(z) = max(\\alpha z, z)$$\n",
    "  - $\\alpha$ is the slope of the function for negative z\n",
    "  - Setting $\\alpha$ to 0.2 (a huge leak) seems to perform better than 0.01 (a small leak)\n",
    "  - Default value of $\\alpha$ = 0.3\n",
    "  - **Preferred if you're worried about runtime latency**\n",
    "  - **Useful if you need your net to be as fast as possible**\n",
    "  \n",
    "RReLU (Randomized Leaky ReLU) \n",
    "  - uses random $\\alpha$ for training, \n",
    "    and a fixed average $\\alpha$ for testing.\n",
    "    It performed fairly well and seemed to act as a regularizer\n",
    "  - **Preferred if you're overfitting**\n",
    "  \n",
    "PReLU (Parametric Leaky ReLU) \n",
    "  - allows $\\alpha$ to be learned during training.\n",
    "    It becomes a parameter that backprop can modify.\n",
    "  - **strongly outperformed ReLU on large datasets, but risks overfitting on smaller datasets**\n",
    "  - **Preferred if you have a huge training set**\n",
    "  \n",
    "ELU (Exponential Linear Unit):\n",
    "  - **Outperformed all ReLUs**\n",
    "  - **Training time was reduced**\n",
    "  - **Network performed better on test set**\n",
    "  - $$\\begin{align}ELU_\\alpha(z) & = \\alpha(e^z - 1) & if \\; z < 0 \\\\\n",
    "                                 & = z & if \\; z >= 0 \\end{align}$$\n",
    "  - ELU increases linearly with slope 1 for z > 0, and\n",
    "    becomes more and more negative with an asymptote at -$\\alpha$ as z becomes more negative\n",
    "  - If $\\alpha\\,=\\,1$, the function is smooth everywhere, including at z = 0.\n",
    "    This allows Gradient Descent to speed up, as it does not bounce left and right of 0.\n",
    "  - **ELU is slower to compute. It's faster training rate helps at training time,\n",
    "    but slow computation hurts during testing.**\n",
    "    \n",
    "SELU (Scaled ELU):\n",
    "  - If:\n",
    "    - Your network consists only of a stack on Dense layers\n",
    "    - All hidden layers use the SELU activation function\n",
    "    - Input features are all standardized (mean 0, std. dev. 1)\n",
    "    - All hidden-layer weights are initialized with LeCun normalization (kernel_initializer = 'lecun_normal')\n",
    "    - Network's architecture must be sequential. No Recurrent Networks, Skip connections (Wide and Deep nets).\n",
    "    - Some researchers have mentioned SELU works well for Convolutional Networks as well (even though there are no Dense layers).\n",
    "  - If there is no regularization (ex. dropout or L1), SELU ensures the model is self-normalized which solves the vanishing/exploding gradients problem\n",
    "**In general, prefer SELU > ELU > Leaky ReLU (and it's variants) > tanh > logistic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you use the ReLU/SELU functions\n",
    "#\n",
    "# model = keras.models.Sequential([\n",
    "#     [ ... ]\n",
    "#     keras.layers.Dense(10, kernel_initializer = \"he_normal\"), \n",
    "#     keras.layers.LeakyReLU(alpha = 0.2),   # For LeakyReLU, this layer should come after each layer you want to\n",
    "#                                            # apply it to. Default alpha = 0.3.\n",
    "#                                            # For PReLU, replace this LeakyReLU() with PReLU().\n",
    "#                                            # No implementation of RReLU() yet - you write your own.\n",
    "#     [ ... ]\n",
    "# ]) \n",
    "\n",
    "# layer = keras.layers.Dense(10, activation='selu', kernel_initializer='lecun_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saturating activation functions\n",
    "\n",
    "Hyperbolic tan function (tanh):\n",
    "  - Useful in the output layer if you need to output a number between -1 and 1\n",
    "  - Rarely used in hidden layers, except for RNNs\n",
    "  \n",
    "Logistic function (Sigmoid):\n",
    "  - Useful in the output layer if you want to output a probability\n",
    "  - Rarely used in hidden layers (except for the coding layer of variational autoencoders)\n",
    "  \n",
    "Softmax function:\n",
    "  - Useful in the output layer to output probabilities for mutually exclusive classes\n",
    "  - Rarely used in hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization (BN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Normalization is one of the most-used layers in deep neural networks. So much so that it is omitted from diagrams and it is assumed that it as there after each layer.**\n",
    "\n",
    "Why do we need it:\n",
    "  - The Glorot/He initialization plus any nonsaturating activation function reduces the danger of vanishing/exploding gradients at the beginning of training. But they can come back during training. BN addresses this issue.\n",
    "\n",
    "How:\n",
    "  - Add BN layer before/after the activation function.\n",
    "\n",
    "How does it work:\n",
    "  - Evaluates mean and std dev of the input over the current mini-batch (hence it's called Batch Normalization).\n",
    "  \n",
    "  $$\\displaystyle\\begin{align}\\mu_B & = \\frac{1}{m_B}\\sum_{i=1}^{m_B}x^{(i)} \\\\\n",
    "                          {\\sigma_B}^2 & = \\frac{1}{m_B}\\sum_{i=1}^{m_B}{(x^{(i)} - \\mu_B)}^2\\end{align}$$\n",
    "  - 0-centers and normalizes each input, then scales and shifts it according to two new parameter vectors per layer.\n",
    "  \n",
    "  $$\\begin{align}\\widehat{x}^{(i)} & = \\frac{x^{(i)}\\;-\\;\\mu_B}{\\sqrt{{\\sigma_B}^2\\;+\\;\\epsilon}} \\\\\n",
    "                           z^{(i)} & = \\gamma\\;\\otimes\\;\\widehat{x}^{(i)}\\;+\\;\\beta\\end{align}$$\n",
    "                           \n",
    "  <paragraph><center>where $\\;\\otimes \\;=\\;$ elementwise$\\;$ multiplication</center></paragraph>\n",
    "  \n",
    "  <paragraph><center>$\\epsilon \\;=\\; $small number to avoid divide-by-zero error, called a smoothing term, typically $10^{-5}$</center></paragraph>\n",
    "  - During backpropagation, each batch-normalized layer learns:\n",
    "    - $\\gamma$ : the output scale vector\n",
    "    - $\\beta$  : the output offset vector\n",
    "    - $\\mu$    : final input mean vector (learned by using a moving average of the layer's input mean)\n",
    "    - $\\sigma$ : final input std dev vector (learned by using a moving average of the layer's input std dev)\n",
    "  - The batch mean/std.dev are used during training, and final mean/std.dev are used after training.\n",
    "  - **Benefits:**\n",
    "    - **Improved neural networks**\n",
    "    - **Strongly reduced vanishing gradients problem**\n",
    "    - **Networks were also much less sensitive to weight initialization**\n",
    "    - **Could use much larger learning rates, speeding up learning**\n",
    "    - **Acts as a regularizer, reducing the need for any other regulatization technique**\n",
    "    - **Does not affect shallower networks as much, but has a tremendous impact on deep networks.**\n",
    "    - **Although converges faster, training per epoch is rather slow. All in all, wall time will be shorter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a Batch Normalization layer before/after each hidden layer's activation function,\n",
    "# and optionally after the first layer in your model (as shown here).\n",
    "#\n",
    "# model = Sequential([  # In this tiny example with just 2 hidden layers, it's unlikely that Batch Normalization\n",
    "#                       # will have a very positive impact. For deeper networks, it can make a tremendous difference.\n",
    "#   Flatten(input_shape=[28, 28]),\n",
    "#   BatchNormalization(),\n",
    "#   Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "#   BatchNormalization(),\n",
    "#   Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "#   BatchNormalization(),\n",
    "#   Dense(10, activation='softmax')\n",
    "# ])\n",
    "# model.summary()  # This will show you the Batch Normalization layers. The number of parameters\n",
    "#                  # added for the BN layers is 4 x (number of input parameters).\n",
    "#                  # mu and sigma are not trainable via backprop, and this is what Keras shows at the bottom.\n",
    "# [(var.name, var.trainable) for var in model.layers[1].variables] # will list trainable/untrainable parameters.\n",
    "#\n",
    "# model.layers[1].updates  # Shows the operations Keras created to train the trainable params (gamma, beta)\n",
    "#                          # at each iteration. Since we're using the Tensorflow backend, these are TF operations.\n",
    "\n",
    "# Shows how to add BN layers before the activation function.\n",
    "# You should try adding them before and after the activation function,\n",
    "# and choosing the way that works best.\n",
    "# model = Sequential([\n",
    "#   Flatten(input_shape=[28, 28]),\n",
    "#   BatchNormalization(),\n",
    "#   Dense(300, kernel_initializer='he_normal', use_bias=False), # Removed activation function.\n",
    "#                                                               # Since BN layer already has bias,\n",
    "#                                                               # remove bias from the layer using use_bias=False.\n",
    "#   BatchNormalization(),\n",
    "#   Activation('elu'),\n",
    "#   Dense(100, kernel_initializer='he_normal', use_bias=False),\n",
    "#   BatchNormalization(),\n",
    "#   Activation('elu'),\n",
    "#   Dense(10, activation='softmax')\n",
    "# ])\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\bf Momentum}$ hyperparameter for BN layer:\n",
    "\n",
    "BN layer has a lot of hyperparameters you can tweak.\n",
    "Most defaults will be fine.\n",
    "Usually you would need to tweak the momentum.\n",
    "Given a new vector of input means or std dev (call it $\\widehat{v}\\,$) computed over the current batch,\n",
    "the BN layer updates the running average $\\widehat{v}$ using:\n",
    "$$\\widehat{v} = \\widehat{v}\\;\\times\\;momentum\\;+\\;v\\;\\times\\;(1 - momentum)$$\n",
    "A good momentum is typically close to 1 (ex. 0.9, 0.9, 0.999). You want more 9's for larger datasets, and smaller mini-batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\bf Axis}$ hyperparameter for BN layer:\n",
    "\n",
    "- Determines which axis should be normalized.\n",
    "- Defaults to -1, which means the last axis. So if you have batch shape [batch size, features], the normalization is across the batch size for each feature.\n",
    "- If BN layer was before the Flatten layer, batch shape would be [batch size, height, width], so normalization would be across the batch size and height for each width. This means we would get 28 means and 28 std devs for each column. If you want to use the 784 pixels independently, you should set axis=[1, 2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "\n",
    "- Gradient Clipping is used in Recurrent Neural Networks (RNN), since Batch Normalization is tricky. Other than RNNs, BN layer works fine.\n",
    "- Gradient Clipping clips the gradient during backprop so it does not exceed some threshold. ex:\n",
    "\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)  # all gradients clipped to $\\pm$1.0\n",
    "\n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "- clipvalue can change the orientation of the gradient vector. ex. if original gradient = [0.9, 100.0], after clipping it will be [0.9, 1.0]\n",
    "- use clipnorm instead of clipvalue to keep orientation of gradient vector. This clips gradient if it's L2 norm > threshold. clipnorm=1.0 will change [0.9, 100.0] to [0.00899, 0.9999], preserving it's orientation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pretrained layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transfer learning: \n",
    "  - Use the lower layers of a DNN trained to solve a similar problem as the first layers of your network. ex. To recognize facial expressions, you can use a DNN that is trained to recognize faces. The lower layers of this network will have learned where the mouth, eyes, etc, are on the face. You can then add your upper layers to recognize facial expressions.\n",
    "  - Transfer learning will work best when the inputs have similar low-level features.\n",
    "  - The more similar the tasks are, the more layers you will want to use. For very similar tasks, keep all hidden layers, and replace just the output layer.\n",
    "- Initially, try locking the weights for the pre-trained layers, so when training the upper layers, those weights remain the same.\n",
    "- If that does not work, try unfreezing one or two top hidden layers to let them updated weights during backprop. The more training data you have, the more layers you can unfreeze. Reduce the learning rate when you unfreeze layers.\n",
    "- If you still have problems:\n",
    "  - and you have little training data, try dropping the top hidden layers and freezing all remaining hidden layers.\n",
    "  - and you have plenty of training data, try replacing the top hidden layers instead of dropping them, and even adding more hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you use a model's weights as-is, any weight changes to the new model will affect the original as well.\n",
    "# So you have to clone the original.\n",
    "# This is just an example. Transfer learning does not work well with small dense networks, probably because\n",
    "# - small networks learn few patterns\n",
    "# - dense networks learn very specific patterns which are unlikely to be useful in other tasks\n",
    "# Transfer learning works best with deep convolutional neural networks, which tend to learn\n",
    "# feature detectors that are much more general (especially in the lower layers).\n",
    "# Deep Convolutional networks in Chapter 14\n",
    "# \n",
    "# model_A = keras.models.load_model('my_model_A.h5')\n",
    "# model_A_clone = keras.models.clone_model(model_A) # model_A is the model that solves a similar problem.\n",
    "# model_A_clone.set_weights(model_A.get_weights())  # This is done since clone_model() does not clone weights.\n",
    "# model_B_on_A = Sequential(model_A_clone.layers[:-1]) # Include all layers except output\n",
    "# model_B_on_A.add(Dense(1, activation='sigmoid'))     # Our own output layer\n",
    "\n",
    "# for layer in model_B_on_A.layers[:-1]:               # Freeze borrowed layer weights. This may be needed for the\n",
    "#     layer.trainable = False                          # first few epochs until the new output layer has learned\n",
    "#                                                      # reasonable weights. This is done since it's weights are \n",
    "#                                                      # random and will wreck the borrowed layer weights.\n",
    "\n",
    "# model_B_on_A.compile(loss='binary_crossentropy',     # You must always compile your model after you\n",
    "#                      optimizer='sgd',                # freeze/unfreeze layers.\n",
    "#                      metrics='accuracy')\n",
    "\n",
    "# history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,         # Train the model for a few epochs\n",
    "#                            validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "# for layer in model_B_on_A.layers[:-1]:               # Now our new layer must have learned good weights, so\n",
    "#     layer.trainable = False                          # unfreeze the lower layers.\n",
    "\n",
    "# optimizer = keras.optimizer.SGD(lr=1e-4)             # After unfreezing the lower layers, it is a good idea to\n",
    "#                                                      # lower the learning rate to avoid damaging the\n",
    "#                                                      # lower-layer weights.\n",
    "\n",
    "# model_B_on_A.compile(loss='binary_crossentropy',     # You must always compile your model after you\n",
    "#                      optimizer=optimizer,            # freeze/unfreeze layers.\n",
    "#                      metrics='accuracy')\n",
    "\n",
    "# history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "#                            validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised pretraining\n",
    "\n",
    "Suppose you have a complex task but not much labeled training data, and cannot find a model trained on a similar task. You should:\n",
    "  - try to get more labeled training data. If you cannot,\n",
    "  - try to perform unsupervised pretraining:\n",
    "    - Requires plenty of unlabeled training data\n",
    "    - Use it to pretrain an unsupervised model (ex. autoencoder/GAN (Generative Adversarial Network)). See Ch 17.\n",
    "    - Use the lower layers of the unsupervised model, add your output layer on top, and fine-tune the final network using supervised learning (with the labeled training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining on an auxiliary task\n",
    "\n",
    "If you don't have much labeled training data, another approach is to:\n",
    "  - train a first network on an auxiliary task for which you have labeled data. Then reuse the lower layers for your actual task.\n",
    "  - ex. for a Natural Language Processing (NLP) task, you can:\n",
    "    - download a corpus of millions of text documents,\n",
    "    - randomly mask out some words and train a model to predict what those words are.\n",
    "    - This will train your model to \"understand\" the language to some extent.\n",
    "    - Then you can reuse it for your actual task and fine-tune it for your labeled data.\n",
    " \n",
    " Self-supervised learning:\n",
    "   - you automatically generate the labels from the data itself.\n",
    "   - train a model on the resulting labeled dataset using supervised learning.\n",
    "   - Does not require human labeling whatsoever, so it is best classified as a form of unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most popular algorithms:\n",
    "  - momentum optimization\n",
    "  - Nesterov Accelerated Gradient\n",
    "  - AdaGrad\n",
    "  - RMSProp\n",
    "  - Adam\n",
    "  - Nadam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum Optimization\n",
    "\n",
    "  - Gradient Descent takes small regular steps down the slope. The step size is also a function of the gradient value. If the gradient is tiny, the step size will be tiny also.\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\eta\\nabla_{\\theta}J(\\theta)$$\n",
    "\n",
    "  - Momentum optimization changes the step size as a function of time, quickly taking larger and larger steps.\n",
    "  \n",
    "$$\\begin{align}m & \\leftarrow\\beta m - \\eta \\nabla_{\\theta}J(\\theta) \\\\\n",
    "          \\theta & \\leftarrow \\theta + m \\end{align}$$\n",
    "          <paragraph><center>where $\\beta$ is the momentum and should be set between 0 (high friction) and 1 (no friction).\n",
    "          If $\\beta$ = 0.9, the velocity is 10 x $\\eta\\nabla_{theta}J(\\theta)$, so\n",
    "          momentum optimization goes 10 times faster than gradient descent.</center></paragraph>\n",
    "  - Benefits:\n",
    "    - escapes from plateaus faster than Gradient Descent\n",
    "    - can roll past local optima\n",
    "    - For deep networks that don't use Batch Normalization, the upper layers will end up with inputs of very different scales. Momentum optimization helps a lot to converge for layers across all input scales.\n",
    "  - Issues:\n",
    "    - The optimizer may overshoot in one direction, then overshoot in another thus oscillating for a while before stabilizing at the minimum. Good to have friction to get rid of oscillations and speed up coverage.\n",
    "  - momentum value of 0.9 usually works well in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient (NAG) AKA Nesterov Momentum Optimization\n",
    "\n",
    "  - This variant of momentum optimization is almost always faster than vanilla momentum optimization. In fact it is significantly faster than regular momentum optimization.\n",
    "  - It measures the gradient of the cost function slightly ahead in the direction of the momentum. The vanilla momentum optimization computes the gradient before the momentum update to $\\theta$. NAG updates the momentum to $(\\theta + \\beta \\; m)$ and then applies the gradient to it. This works because in general the momentum will point in the right direction.\n",
    "  \n",
    "  $$\\begin{align}m & \\leftarrow\\beta m - \\eta \\nabla_{\\theta}J(\\theta + \\beta \\; m) \\\\\n",
    "          \\theta & \\leftarrow \\theta + m \\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = kears.optimizers.SGD(lr=0.0001, momentum=0.9, nesterov=True)  # Nesterov Accelerated Gradient (NAG), \n",
    "#                                                                          # AKA Nesterov Momentum Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "  - It would be nice if the algorithm could correct it's direction to point more towards the global optimum. AdaGrad achieves this by scaling down the gradient vector along the steepest dimensions.\n",
    "\n",
    "$$\\begin{align}s & \\leftarrow s + \\nabla_{\\theta}J(\\theta) \\otimes \\nabla_{\\theta} J(\\theta) \\\\\n",
    "         \\theta  & \\leftarrow \\theta - \\eta \\nabla_{\\theta} J(\\theta) \\div \\sqrt{s + \\epsilon} \\end{align}$$\n",
    "         <paragraph><center>Accululate the square of the gradient into the variable s.\n",
    "    Then scale down the gradient vector by a factor of $\\sqrt{s + \\epsilon}$.\n",
    "    Let's use the $\\div$ symbol for elementwise division.\n",
    "    $\\epsilon$ is a smoothing term to avoid div-by-zero. Typically set to 1e-10. This decays the learning rate faster for steeper dimensions than for dimensions with gentler slopes. This is called an adaptive learning rate. </center></paragraph>\n",
    "  - Benefits:\n",
    "    - It helps point the update toward the global optimum.\n",
    "    - This algorithm requires much less tuning of the learning rate $\\eta$\n",
    "    - Performs well for simple quadratic problems - may be efficient for Linear Regression.\n",
    "  - Issues:\n",
    "    - Often stops too early. The learning rate gets scaled down so much that the algorithm stops before reaching global optimum. You should not use it for Deep NNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "\n",
    "  - Since AdaGrad runs the risk of slowing down a bit too fast and never converging, RMSProp accumulates the gradients from only the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in during gradient accumulation (first equation below):\n",
    "\n",
    "$$\\begin{align}s & \\leftarrow \\beta s + (1 - \\beta) \\nabla_{\\theta}J(\\theta) \\otimes \\nabla_{\\theta} J(\\theta) \\\\\n",
    "         \\theta  & \\leftarrow \\theta - \\eta \\nabla_{\\theta} J(\\theta) \\odiv \\sqrt{s + \\epsilon)} \\end{align}$$\n",
    "         <paragraph><center>Accumulate the square of the latest gradients into the variable s.\n",
    "    Then scale down the gradient vector by a factor of $\\sqrt{s + \\epsilon}$.\n",
    "    The $\\odiv$ is the symbol for elementwise division.\n",
    "    The decay rate is $\\beta$.\n",
    "    $\\epsilon$ is a smoothing term to avoid div-by-zero. Typically set to 1e-10. This decays the learning rate faster for steeper dimensions than for dimensions with gentler slopes. This is called an adaptive learning rate. </center></paragraph>\n",
    "\n",
    "  - The decay rate $\\beta$ is typically set to 0.9 (which typically works well).\n",
    "  - Can lead to solutions that generalize poorly on some datasets. So if you're seeing this, try using plain Nesterov Accelerated Gradient instead.\n",
    "  - RMSProp was the preferred optimization algorithm for many researchers until Adam optimization came around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9) # rho is the decay rate beta in the above text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam, AdaMax and Nadam Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Adam (Adaptive Moment Estimation) combines the ideas of momentum optimization and RMSProp.\n",
    "    - It keeps track of exponentially decaying average of past gradients\n",
    "    - Exponentially decaying average of past squared gradients\n",
    "    \n",
    " $$\\begin{align}m & \\leftarrow \\beta_1 m - (1 - \\beta_1) \\nabla_{\\theta} J(\\theta) \\\\\n",
    "                s & \\leftarrow \\beta_2 s + (1 - \\beta_2) \\nabla_{\\theta}J(\\theta) \\otimes \\nabla_{\\theta} J(\\theta) \\\\\n",
    "      \\widehat{m} & \\leftarrow \\frac{m}{1 - \\beta_1^T} \\\\\n",
    "      \\widehat{s} & \\leftarrow \\frac{s}{1 - \\beta_2^T} \\\\\n",
    "           \\theta & \\leftarrow \\theta + \\;\\eta \\; \\widehat{m} \\; \\odiv \\sqrt{\\widehat{s} + \\epsilon}\n",
    "      \\end{align}$$\n",
    "      <paragraph><center>T represents the iteration number starting at 1.\n",
    "    $\\widehat{m}$ and $\\widehat{s}$ are initialized to 0 at the beginning of training, so the third and fourth equations are needed to boost them.</center></paragraph>\n",
    "    - Momentum decay $\\beta_1$ is typically initialized to 0.9\n",
    "    - Scaling decay $\\beta_2$ is typically initialized to 0.999\n",
    "    - Smoothing term $\\epsilon$ defaults to None, which tells Keras to use keras.backend.epsilon which is 1e-7. If you want, you can change it using keras.backend.set_epsilon()\n",
    "    - Learning rate $\\eta$ is typically set to 0.001. Since Adam is adaptive, it requires less tuning of $\\eta$\n",
    "    - Can lead to solutions that generalize poorly on some datasets. So if you're seeing this, try using plain Nesterov Accelerated Gradient instead.\n",
    "  - AdaMax\n",
    "    - replaces the L2 norm of parameter updates of Adam with the $L_{\\infty}$ norm (the max value of the time-decayed gradients).\n",
    "    - Adam performs better, so you can try this algorithm if you're seeing problems with AdaMax\n",
    "  - Nadam (Nesterov Adam)\n",
    "    - This is Adam + Nesterov trick, so it will converge slightly faster than Adam.\n",
    "    - Can lead to solutions that generalize poorly on some datasets. So if you're seeing this, try using plain Nesterov Accelerated Gradient instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Sparse Models\n",
    "\n",
    "- If you need\n",
    "  - blazing fast model at runtime, or\n",
    "  - you need to take up less memory\n",
    "- then\n",
    "  - train the model as usual\n",
    "  - set it's tiny weights to 0 (using model.set_weights()?)\n",
    "  - This may typically not lead to a very sparse model, and\n",
    "  - may degrade the model's performance.\n",
    "- Better option\n",
    "  - Apply strong L1 regularization during training (this zeroes out many weights)\n",
    "- If this is not enough\n",
    "  - Use TensorFlow Model Optimization Toolkit (TF-MOT) which has a pruning API that iteratively removes connections during training based on their magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untested code - check to make sure it works.\n",
    "# To set model weights to zero for weights less than threshold.\n",
    "\n",
    "# def set_low_weights_to_zero(layer, threshold=0.01):\n",
    "#     weights = layer.get_weights()\n",
    "#     weights[weights < threshold] = 0.0\n",
    "#     layer.set_weights(weights)\n",
    "\n",
    "# layer = model.layers[1]\n",
    "# set_low_weights_to_zero(layer, threshold=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Comparison\n",
    "\n",
    "<table>\n",
    "    <caption>Optimizer Comparison</caption>\n",
    "    <tr><th>Class</th><th>Convergence Speed</th><th>Convergence quality</th></tr>\n",
    "    <tr><td>SGD</td><td>*</td><td>**</td></tr>\n",
    "    <tr><td>SGD(momentum=...)</td><td>**</td><td>***</td></tr>\n",
    "    <tr><td>SGD(momentum=..., nesterov=True)</td><td>**</td><td>***</td></tr>\n",
    "    <tr><td>Adagrad</td><td>***</td><td>* (stops too early)</td></tr>\n",
    "    <tr><td>RMSProp</td><td>***</td><td>** or ***</td></tr>\n",
    "    <tr><td>Adam</td><td>***</td><td>** or ***</td></tr>\n",
    "    <tr><td>Nadam</td><td>***</td><td>** or ***</td></tr>\n",
    "    <tr><td>AdaMax</td><td>***</td><td>** or ***</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - To find a good rough LR:\n",
    "    - You can train your model at different exponential learning rates (say 0.001, 0.01, 0.1, 1.0).\n",
    "    Then pick an LR that gives fast convergence without blowing up\n",
    "  - Then you can pick an LR that is slightly lower than the best LR you picked above, and train your model. If you're satistifed with the error, this is fine. If you're not satisfied:\n",
    "    - Select a range around the best rough LR you found above, and zoom in by trying at various LRs around it.\n",
    "    - If you're satistifed with the loss, you can stop. If not, continue to zoom into the LR that gives you the error threshold you're trying to achieve. You may not know the threshold - in which case you can zoom in to see which LR is best.\n",
    "  - Learning Schedules:\n",
    "    - Power scheduling\n",
    "    \n",
    "        $$\\eta(t) = \\frac{\\eta_0}{1 + \\left(\\frac{t}{s}\\right)^c}$$\n",
    "\n",
    "      <paragraph><center>t = iteration number <br/>\n",
    "      $\\eta_0$ = initial learning rate <br/>\n",
    "      c = power (typically 1) <br/>\n",
    "      s = number of steps before learning rate drops. After s steps it is down to $\\eta_0/2$, after another s steps it is down to $\\eta_0/3$, etc. <br/></center></paragraph>\n",
    "    - Exponential scheduling\n",
    "        $$\\eta(t) = \\eta_0 \\; {0.1}^{\\frac{t}{s}}$$\n",
    "      <paragraph><center>t = iteration number <br/>\n",
    "      $\\eta_0$ = initial learning rate <br/>\n",
    "      c = power (typically 1) <br/>\n",
    "      s = number of steps before learning rate drops. Slashes LR by a factor of 10 every s steps <br/></center></paragraph>\n",
    "      - Works well, converges fast\n",
    "      - Easy to tune\n",
    "      - 1cycle is best\n",
    "    - Piecewise constant scheduling\n",
    "      - Use a constant rate of say $\\eta_0$ = 0.1 for 5 epochs, then a smaller rate of say $\\eta_0$ = 0.01 for 50 epochs, etc. Requires fiddling around with the right sequence and number of epochs.\n",
    "    - Performance scheduling\n",
    "      - Measure validation error every N steps (just like for early stopping), then reduce LR by a factor of $\\lambda$ when the error stops dropping\n",
    "      - Works well, converges fast\n",
    "      - Easy to tune\n",
    "    - 1cycle scheduling\n",
    "      - Choose an $\\eta_1$ based on the rough LR algorithm shown above\n",
    "      - Choose $\\eta_0$ to be 0.1 $\\eta_1$\n",
    "      - Increase LR from $\\eta_0$ to $\\eta_1$ linearly halfway through training (half of the epochs).\n",
    "        If we're using momentum, start with a high momentum (say 0.95), then drop it linearly to say 0.85.\n",
    "      - Decrease LR from $\\eta_1$ to $\\eta_0$ linearly through the rest of training.\n",
    "        If we're using momentum, increase momentum from the 0.85 level to 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the scheduling functions, the final LR is saved with the model \n",
    "# unless your schedule function uses the epoch argument.\n",
    "# In this case, when you load your model and want to continue training,\n",
    "# you would set the fit() method's initial_epoch argument so the epoch starts\n",
    "# at the right value.\n",
    "\n",
    "# power scheduling\n",
    "# optimizer = keras.optimizer.SGD(lr=0.01, decay=1e-4)  # decay is inverse of s.\n",
    "                                                        # Keras assumes c = 1.\n",
    "\n",
    "# exponential scheduling\n",
    "# def exponential_decay(lr0, s):       # When you create a LearningRateScheduler instance with the exponential_decay_fn,\n",
    "#     def exponential_decay_fn(epoch): # the LearningRateScheduler will update the optimizer's LR at the beginning \n",
    "#         return lr0 * 0.1**(epoch / s)# of each epoch. You can also write your own callback to update LR at each step.\n",
    "#     return exponential_decay_fn      # This makes sense if there are many steps per epoch.\n",
    "#                                      # Alternatively, use keras.optimizers.schedules.\n",
    "#                                      # The exponential_decay_fn() could take the LR as the second argument to return\n",
    "#                                      # an updated LR value.\n",
    "# exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
    "\n",
    "# lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "# history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])\n",
    "\n",
    "# # piecewise constant scheduling\n",
    "# def piecewise_constant_fn(epoch):\n",
    "#     if epoch < 5:\n",
    "#         return 0.01\n",
    "#     elif epoch < 15:\n",
    "#         return 0.005\n",
    "#     else:\n",
    "#         return 0.001\n",
    "    \n",
    "# # performance scheduling\n",
    "# lr_scheduler = keras.callback.ReduceLROnPlateau(factor=0.5, patience=5) # Multiply LR by 0.5 if validation\n",
    "#                                                                         # error is the same for 5 consecutifve epochs\n",
    "#                                                                         # (given by the patience variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To update the LR at each step (rather than at each epoch), \n",
    "# use the tf.keras.optimizers.schedules.\n",
    "# This is specific to tf.keras - it is not available in the Keras API.\n",
    "# The example below does the same thing as the exponential_decay_fn() above.\n",
    "#\n",
    "# s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
    "# learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "# optimizer = keras.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1cycle scheduling.\n",
    "# This is how you would scan through the LR range to find the LR value that gives the minimum loss.\n",
    "\n",
    "# K = keras.backend\n",
    "\n",
    "# class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "#     def __init__(self, factor):\n",
    "#         self.factor = factor\n",
    "#         self.rates = []\n",
    "#         self.losses = []\n",
    "#     def on_batch_end(self, batch, logs):\n",
    "#         self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "#         self.losses.append(logs[\"loss\"])\n",
    "#         K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "\n",
    "# def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "#     init_weights = model.get_weights()\n",
    "#     iterations = len(X) // batch_size * epochs\n",
    "#     factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "#     init_lr = K.get_value(model.optimizer.lr)\n",
    "#     K.set_value(model.optimizer.lr, min_rate)\n",
    "#     exp_lr = ExponentialLearningRate(factor)\n",
    "#     history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "#                         callbacks=[exp_lr])\n",
    "#     K.set_value(model.optimizer.lr, init_lr)\n",
    "#     model.set_weights(init_weights)\n",
    "#     return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "# def plot_lr_vs_loss(rates, losses):\n",
    "#     plt.plot(rates, losses)\n",
    "#     plt.gca().set_xscale('log')\n",
    "#     plt.hlines(min(losses), min(rates), max(rates))\n",
    "#     plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "#     plt.xlabel(\"Learning rate\")\n",
    "#     plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(42)\n",
    "# np.random.seed(42)\n",
    "\n",
    "# model = keras.models.Sequential([\n",
    "#     keras.layers.Flatten(input_shape=[28, 28]),\n",
    "#     keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "#     keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "#     keras.layers.Dense(10, activation=\"softmax\")\n",
    "# ])\n",
    "# model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "#               optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "#               metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 128\n",
    "# rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "# plot_lr_vs_loss(rates, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows how you can ramp up on the LR rate and ramp down on it at the halfway point.\n",
    "#\n",
    "\n",
    "# class OneCycleScheduler(keras.callbacks.Callback):\n",
    "#     def __init__(self, iterations, max_rate, start_rate=None,\n",
    "#                  last_iterations=None, last_rate=None):\n",
    "#         self.iterations = iterations\n",
    "#         self.max_rate = max_rate\n",
    "#         self.start_rate = start_rate or max_rate / 10\n",
    "#         self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "#         self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "#         self.last_rate = last_rate or self.start_rate / 1000\n",
    "#         self.iteration = 0\n",
    "#     def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "#         return ((rate2 - rate1) * (iter2 - self.iteration)\n",
    "#                 / (iter2 - iter1) + rate1)\n",
    "#     def on_batch_begin(self, batch, logs):\n",
    "#         if self.iteration < self.half_iteration:\n",
    "#             rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "#         elif self.iteration < 2 * self.half_iteration:\n",
    "#             rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "#                                      self.max_rate, self.start_rate)\n",
    "#         else:\n",
    "#             rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "#                                      self.start_rate, self.last_rate)\n",
    "#             rate = max(rate, self.last_rate)\n",
    "#         self.iteration += 1\n",
    "#         K.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epochs = 25\n",
    "# onecycle = OneCycleScheduler(len(X_train) // batch_size * n_epochs, max_rate=0.05)\n",
    "# history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "#                     validation_data=(X_valid_scaled, y_valid),\n",
    "#                     callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Deep Neural Networks have millions of parameters. So it is easy to overfit when using them. We need to regularize.\n",
    "  - Techniques for regularization:\n",
    "    - Early Stopping\n",
    "    - L1 and L2 regularization\n",
    "    - Dropout regularization\n",
    "    - Max-norm regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 and L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 regularization may create a sparse model (many weights are 0)\n",
    "# L2 regularization constraints the connection weights\n",
    "\n",
    "# layer = Dense(100, activation='elu',\n",
    "#               kernel_initializer='he_normal',\n",
    "#               kernal_regularizer=keras.regularizers.l2(0.01)) # To use L1 regularization, use l1(0.01).\n",
    "#                                                               # To use both L1 and L2 regularization, use:\n",
    "#                                                               # keras.regularizers.l1_l2(l1=0.01, l2=0.01)\n",
    "\n",
    "# # Typically, you use the same regularizer for all layers, and the same init strategy for all layers.\n",
    "# # Instead of repeating code, use functools.partial():\n",
    "\n",
    "# from functools import partial\n",
    "\n",
    "# RegularizedDense = partial(keras.layers.Dense,\n",
    "#                            activation='elu',                   # Put all your common layer init here\n",
    "#                            kernel_initializer='he_normal',\n",
    "#                            kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "# model = Sequential([\n",
    "#     Flatten(input_shape=[28, 28]),\n",
    "#     RegularizedDense(300),\n",
    "#     RegularizedDense(100),\n",
    "#     RegularizedDense(10, activation='softmax', kernel_initializer='glorot_uniform') # Setting other arguments \n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Regularization\n",
    "\n",
    "  - Even a state of the art network gets a 1-2% accuracy boost by adding dropout.\n",
    "    If you're already at 95% accuracy, 2% boost means error went down by 40%. This is huge.\n",
    "  - Dropout algorithm allows each neuron a probability p of getting dropped out at each training step.\n",
    "    If dropped, the neuron will be entirely ignored during this training step.\n",
    "    It may become active in the next training step.\n",
    "  - p = dropout rate, typically 0.1 - 0.5 (closer to 0.2-0.3 in RNNs, closer to 0.4-0.5 in CNNs)\n",
    "  - After training, neurons don't get dropped anymore\n",
    "  - Input neurons could be dropped\n",
    "  - Output neurons are never dropped\n",
    "  - A neuron has to not rely on any single input neuron, since it can be dropped out.\n",
    "    Neurons thus become less sensitive to small changes in the input - they generalize better.\n",
    "  - Another way to think about it:\n",
    "    - At each training step, the network is different\n",
    "    - If there are N possible neurons that can be dropped, there are $2^N$ possible networks\n",
    "    - If you've run 10000 training steps, you've tried 10000 different networks (each with just one training instance).\n",
    "    - The resulting network can be seen as an average ensemble of these networks\n",
    "  - If p = 0.5 during training, a neuron is connected to 2 x as many neurons during testing. This is because during testing we don't drop neurons. This is why we have to compensate by multiplying each neuron's weight by 0.5. More generally, we need to multiply each neuron's output weight by the keep probability (1 - p) after training. Instead we can divide each neuron's output by the keep probability during training. The Dropout layer handles this for us.\n",
    "  - In practice you drop neurons from only the top 1-3 layers (ignoring the output layer)\n",
    "  - Slows convergence during training, by approximately a factor of 2.\n",
    "  - It is only turned on during training, so no impact on inference speed.\n",
    "  - Results in a much better model - worth the time and effort\n",
    "  - For SELU activation function, use alpha dropout (it preserves mean and std dev of it's inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential([\n",
    "#     Flatten(input_shape=[28, 28]),\n",
    "#     Dropout(rate=0.2),\n",
    "#     Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "#     Dropout(rate=0.2),\n",
    "#     Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "#     Dropout(rate=0.2),\n",
    "#     Dense(10, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# Since neurons are missing due to dropout during training, and are present during validation,\n",
    "# the training and validation losses cannot be compared.\n",
    "# The model could be overfitting the training set, but have the same validation losses.\n",
    "# To compare them, evaluate the training loss without dropout (i.e. after training).\n",
    "# If full dropout is too strong, you can try dropout for only one layer (the one before the output)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo (MC) Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Can boost the performance of any trained model without retraining or modifying it\n",
    "  - Provides a much better measure of the model's uncertainity\n",
    "  - Simple to implement\n",
    "  - MC Dropout is active during training (so it slows down training) and inference (slows down inference slightly).\n",
    "    Usually during MC Dropout, you want to run inference 10 or more times to get better predictions.\n",
    "    Due to this, predictions are slowed down by a factor of 10 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_probas = np.stack([model.predict(X_test_scaled, training=True)# With 10000 instances in the test set and 10 classes,\n",
    "#                      for sample in range(100)])                 # model.predict() returns matrix of shape [10000, 10].\n",
    "# y_proba = y_probas.mean(axis=0)                                 # Stacking 100 such matrices results in an array of\n",
    "#                                                                 # shape [100, 10000, 10]. Averaging over axis=0, gives\n",
    "#                                                                 # shape [10000, 10] - a single prediction.\n",
    "# # Averaging over a multiple predictions with dropout on (we have training=True) \n",
    "# # gives us a Monte Carlo estimate\n",
    "# # that is more reliable than a single prediction with dropout off.\n",
    "# # You can compare the y_probas values against the first instance prediction:\n",
    "# np.round(model.predict(X_test_scaled[:1]), 2) # versus\n",
    "# np.round(y_probas[:, :1], 2)\n",
    "\n",
    "# To see what the first instance average prediction using MC dropout is:\n",
    "# np.round(y_proba[:1], 2)\n",
    "\n",
    "# Also check out the std dev of the probability estimates.\n",
    "# If there is a large variance in the probability estimates,\n",
    "# you may treat these predictions warily (based on if they\n",
    "# were life threatening, so cricital in some other way.)\n",
    "#\n",
    "# y_std = y_probas.std(axis=0)\n",
    "# np.round(y_std[:1], 2)\n",
    "\n",
    "# The higher the number of samples you use (100 in this example),\n",
    "# the more accurate the predictions and their uncertainity estimates.\n",
    "# But inference time will also increase.\n",
    "\n",
    "# If you have other layers in your model that behave in a special way (ex. BatchNormalization),\n",
    "# then you should not force training mode as we did in the model.predict() function above.\n",
    "# Instead, replace Dropout layers with the MCDropout class shown below.\n",
    "# This will ensure that only the dropout layers force training=True.\n",
    "\n",
    "# class MCDropout(Dropout):\n",
    "#     def call(self, inputs):\n",
    "#         return super().call(inputs, training=True)\n",
    "    \n",
    "# If your model was already trained using Dropout,\n",
    "# - create a new model, identical to the original, except:\n",
    "#   it replaces the Dropout layers with MCDropout.\n",
    "# - copy the existing model's weights to your new model\n",
    "# - run the prediction on the model and average the output as\n",
    "#   shown earlier in this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Norm Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Each neuron's weights are constrained such that $\\;||w||_2 \\le r$\n",
    "  - Implemented by computing the weight norm after each training step and rescaling $w \\leftarrow w \\frac{r}{||w||_2}$\n",
    "  - Reducing r increases regularization.\n",
    "  - Max-Norm Regularization can also alleviate the unstable gradients problems (if you're not using Batch Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense(100, activation='elu', kernel_initializer='he_normal',\n",
    "#       kernal_constraint=keras.constraints.max_norm(1.))       # The value given to max_norm() is the r value above.\n",
    "#                                                               # The axis value for max_norm() defaults to 0.\n",
    "#                                                               # Dense layer weights shape=[num_inputs, num_neurons].\n",
    "#                                                               # So axis=0 means max-norm constraint applies to each\n",
    "#                                                               # neuron's weight vector.\n",
    "#                                                               # For CNNs, set max_norm()'s axis appropriately -\n",
    "#                                                               # Usually axis=[0, 1, 2].\n",
    "\n",
    "# You can define your own function as the kernel_constraint.\n",
    "# You can also constraint the bias terms using bias_constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Practical Guidelines\n",
    "\n",
    "The top table below works in most cases. Please do not consider these defaults as hard rules though !!\n",
    "<table>\n",
    "    <caption>Default DNN Configuration</caption>\n",
    "    <th>Hyperparameter</th><th>Default value</th>\n",
    "    <tr><td>Kernel initializer</td><td>He initialization</td></tr>\n",
    "    <tr><td>Activation function</td><td>ELU</td></tr>\n",
    "    <tr><td>Normalization</td><td>None if shallow; Batch Norm if deep</td></tr>\n",
    "    <tr><td>Regularization</td><td>Early stopping (+L2 reg. if needed)</td></tr>\n",
    "    <tr><td>Optimizer</td><td>Momentum optimization (or RMSProp or Nadam)</td></tr>\n",
    "    <tr><td>Learning rate schedule</td><td>1cycle</td></tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "    <caption>DNN Configuration for a self-normalizing net</caption>\n",
    "    <th>Hyperparameter</th><th>Default value</th>\n",
    "    <tr><td>Kernel initializer</td><td>LeCun initialization</td></tr>\n",
    "    <tr><td>Activation function</td><td>SELU</td></tr>\n",
    "    <tr><td>Normalization</td><td>None (self-normalization)</td></tr>\n",
    "    <tr><td>Regularization</td><td>Alpha dropout if needed</td></tr>\n",
    "    <tr><td>Optimizer</td><td>Momentum optimization (or RMSProp or Nadam)</td></tr>\n",
    "    <tr><td>Learning rate schedule</td><td>1cycle</td></tr>\n",
    "</table>\n",
    "\n",
    "  - Normalize the input features\n",
    "  - If you find a pre-trained net that solves a similar problem, re-use it\n",
    "  - Use unsupervised pre-training if you have a lot of unlabeled data\n",
    "  - Use pre-training on an auxiliary task if you have a lot of labeled data for a similar task\n",
    "  - Exceptions to the above guidelines:\n",
    "    - Use L1 regularization to get a sparse model (optionally zero out the tiny weights after training)\n",
    "      For an even sparser model, use TensorFlow Model Optimization Toolkit (TF-MOT).\n",
    "      This will break self-normalization, so you should use the default configuration.\n",
    "    - For a low-latency model (to perform lightning-fast predictions):\n",
    "      - use fewer layers\n",
    "      - fold the BN layers into the previous layers\n",
    "      - use a faster activation function (ex. leaky ReLU or ReLU)\n",
    "      - use a sparse model\n",
    "      - reduce the float precision from 32-bits to 16-bits or 8-bits\n",
    "      - use TensorFlow Model Optimization Toolkit (TF-MOT)\n",
    "    - If you're building a risk-sensitive app, or latency is not important:\n",
    "      - use MC dropout to boost performance and get more reliable probability estimates along with uncertainity estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
