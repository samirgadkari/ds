{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A large portion of this code is taken from Aur√©lien G√©ron's: Hands-On machine learning with SciKit-Learn, Keras and Tensorflow (2nd edition). I have put comments based on information in the book as well as information I found elsewhere.\n",
    "\n",
    "## Chapter 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding gradients problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanishing gradients:\n",
    "  - If the gradients at the upper layer are small, they're divided and assigned to each neuron in the lower layer.\n",
    "    These values are then further divided and assigned to each neron in the layer below it.\n",
    "    As the number of layers are large in a Deep Neural Network, the gradients at the bottom are zero.\n",
    "    During forward propagation, this causes the weights to remain the same and training never converges.\n",
    "Exploding gradients:\n",
    "  - If the loss function has a steep gradient at the location where you're computing it,\n",
    "    there will be a large change in the weight values. This in turn can cause you to overshoot the minima\n",
    "    and land on the other side of the loss function. If the gradient at that point is also large,\n",
    "    this can cause another large change in the weight values, and another overshoot.\n",
    "  - Sometimes these overshoots will get larger and larger getting you more and more away from the minima.\n",
    "    Sometimes these overshoots will get you into a region of the loss function where the gradients are smaller,\n",
    "    thus temporarily keeping the changes to the weights low. But if you get back down to where the\n",
    "    gradients are large, you may find yourself again overshooting. This will cause you to oscillate,\n",
    "    and never get to the minima.\n",
    "  - More generally, neural networks suffer from unstable gradients. \n",
    "    Different layers may learn at widely different speeds.\n",
    "\n",
    "These problems result from using the Sigmoid activation function, \n",
    "and the Standard Normal initial weight distribution\n",
    "(0 mean, 1 std. dev.). With this initialization, \n",
    "the input variance for each layer increases at the output of that layer. \n",
    "This causes the top layer's sigmoid function to get input that saturates it's output.\n",
    "With a saturated output, the gradient is close to zero.\n",
    "Backpropagation has no gradient to propagate back,\n",
    "any small gradient keeps getting diluted at each lower layer.\n",
    "The Sigmoid function has a mean of 0.5, which exacerbates the problem.\n",
    "The Hyperbolic tangent function behaves better (since it's mean is 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glorot and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution is to use Glorot initialization:\n",
    "$$fan_{avg} = \\frac{fan_{in} + fan_{out}}{2}$$\n",
    "Initialize weights with Normal distribution with 0 mean, variance $\\sigma^2 = \\frac{1}{fan_{avg}}$\n",
    "\n",
    "Or, a uniform distribution between $\\pm$r, with $r = \\sqrt{\\frac{3}{fan_{avg}}} = \\sqrt{3{\\sigma}^2}$\n",
    "\n",
    "Glorot initialization can speed up training considerably.\n",
    "\n",
    "| Initialization | Activation functions | $\\sigma^2$ (Normal) |\n",
    "|---|---|---|\n",
    "|Glorot|None, tanh, logistic, softmax|$\\frac{1}{fan_{avg}}$|\n",
    "|He|ReLU and variants|$\\frac{2}{fan_{in}}$|\n",
    "|LeCun|SELU|$\\frac{1}{fan_{in}}$|\n",
    "\n",
    "By default Keras uses Glorot initialization with a uniform distribution.\n",
    "To use He init with $fan_{in}$:\n",
    "  - keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal') # or kernel_initializer='he_uniform'\n",
    "\n",
    "To use He init with $fan_{avg}$:\n",
    "  - he_avg_init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg', distribution='uniform')\n",
    "  - keras.layers.Dense(10, activation='sigmoid', kernel_initializer=he_avg_init)\n",
    "  - With distribution=\"uniform\", samples are drawn from a uniform distribution within [-limit, limit], with \n",
    "    - limit = sqrt(3 * scale / n).\n",
    "    - n = number of input weights\n",
    "  - So you can specify the limits of the uniform distribution by setting the scale value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default Keras uses Glorot initialization with a uniform distribution. To use He init with ùëìùëéùëõ_ùëñùëõ:\n",
    "\n",
    "# keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal') \n",
    "# or kernel_initializer='he_uniform'\n",
    "\n",
    "# To use He init with ùëìùëéùëõ_ùëéùë£ùëî:\n",
    "# he_avg_init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg', distribution='uniform')\n",
    "# keras.layers.Dense(10, activation='sigmoid', kernel_initializer=he_avg_init)\n",
    "\n",
    "# With distribution=\"uniform\", samples are drawn from a uniform distribution within [-limit, limit], with\n",
    "#     limit = sqrt(3 * scale / n).\n",
    "#     n = number of input weights\n",
    "# So you can specify the limits of the uniform distribution by setting the scale value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU:\n",
    "  - $$\\begin{align}RELU(z) & = 0 \\;\\;\\; if \\; z < 0 \\\\\n",
    "                       & = z \\;\\;\\; if \\; z >= 0 \\end{align}$$                   \n",
    "  - ReLU function thus does not saturate for positive values.\n",
    "  - It is also fast to compute, since we only have to look at\n",
    "    z to decide the output.\n",
    "  - If a layer's weighted sums are negative for all input instances,\n",
    "    the output is 0 (and stays 0 forever). This is the dying ReLU problem.\n",
    "  - Use Leaky ReLU instead\n",
    "  \n",
    "Leaky ReLU:\n",
    "  - $$Leaky \\; ReLU_\\alpha(z) = max(\\alpha z, z)$$\n",
    "  - $\\alpha$ is the slope of the function for negative z\n",
    "  - Setting $\\alpha$ to 0.2 (a huge leak) seems to perform better than 0.01 (a small leak)\n",
    "  - Default value of $\\alpha$ = 0.3\n",
    "  - **Preferred if you're worried about runtime latency**\n",
    "  \n",
    "RReLU (Randomized Leaky ReLU) \n",
    "  - uses random $\\alpha$ for training, \n",
    "    and a fixed average $\\alpha$ for testing.\n",
    "    It performed fairly well and seemed to act as a regularizer\n",
    "  - **Preferred if you're overfitting**\n",
    "  \n",
    "PReLU (Parametric Leaky ReLU) \n",
    "  - allows $\\alpha$ to be learned during training.\n",
    "    It becomes a parameter that backprop can modify.\n",
    "    It strongly outperformed ReLU on large datasets, but risks overfitting on smaller datasets.\n",
    "  - **Preferred if you have a huge training set**\n",
    "  \n",
    "ELU (Exponential Linear Unit):\n",
    "  - Outperformed all ReLUs\n",
    "  - Training time was reduced\n",
    "  - Network performed better on test set\n",
    "  - $$\\begin{align}ELU_\\alpha(z) & = \\alpha(e^z - 1) & if \\; z < 0 \\\\\n",
    "                                 & = z & if \\; z >= 0 \\end{align}$$\n",
    "  - ELU increases linearly with slope 1 for z > 0, and\n",
    "    becomes more and more negative with an asymptote at -$\\alpha$ as z becomes more negative\n",
    "  - If $\\alpha\\,=\\,1$, the function is smooth everywhere, including at z = 0.\n",
    "    This allows Gradient Descent to speed up, as it does not bounce left and right of 0.\n",
    "  - ELU is slower to compute. It's faster training rate helps at training time,\n",
    "    but slow computation hurts during testing.\n",
    "    \n",
    "SELU (Scaled ELU):\n",
    "  - If:\n",
    "    - Your network consists only of a stack on Dense layers\n",
    "    - All hidden layers use the SELU activation function\n",
    "    - Input features are all standardized (mean 0, std. dev. 1)\n",
    "    - All hidden-layer weights are initialized with LeCun normalization (kernel_initializer = 'lecun_normal')\n",
    "    - Network's architecture must be sequential. No Recurrent Networks, Skip connections (Wide and Deep nets).\n",
    "    - Some researchers have mentioned SELU works well for Convolutional Networks as well (even though there are no Dense layers).\n",
    "    \n",
    "**In general, prefer SELU > ELU > Leaky ReLU (and it's variants) > tanh > logistic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you use the ReLU/SELU functions\n",
    "#\n",
    "# model = keras.models.Sequential([\n",
    "#     [ ... ]\n",
    "#     keras.layers.Dense(10, kernel_initializer = \"he_normal\"), \n",
    "#     keras.layers.LeakyReLU(alpha = 0.2),   # For LeakyReLU, this layer should come after each layer you want to\n",
    "#                                            # apply it to. Default alpha = 0.3.\n",
    "#                                            # For PReLU, replace this LeakyReLU() with PReLU().\n",
    "#                                            # No implementation of RReLU() yet - you write your own.\n",
    "#     [ ... ]\n",
    "# ]) \n",
    "\n",
    "# layer = keras.layers.Dense(10, activation='selu', kernel_initializer='lecun_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization (BN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization is one of the most-used layers in deep neural networks. So much so that it is omitted from diagrams and it is assumed that it as there after each layer.\n",
    "\n",
    "Why do we need it:\n",
    "  - The Glorot/He initialization plus any nonsaturating activation function reduces the danger of vanishing/exploding gradients at the beginning of training. But they can come back during training. BN addresses this issue.\n",
    "\n",
    "How:\n",
    "  - Add BN layer before/after the activation function.\n",
    "\n",
    "How does it work:\n",
    "  - Evaluates mean and std dev of the input over the current mini-batch (hence it's called Batch Normalization).\n",
    "  \n",
    "  $$\\displaystyle\\begin{align}\\mu_B & = \\frac{1}{m_B}\\sum_{i=1}^{m_B}x^{(i)} \\\\\n",
    "                          {\\sigma_B}^2 & = \\frac{1}{m_B}\\sum_{i=1}^{m_B}{(x^{(i)} - \\mu_B)}^2\\end{align}$$\n",
    "  - 0-centers and normalizes each input, then scales and shifts it according to two new parameter vectors per layer.\n",
    "  \n",
    "  $$\\begin{align}\\widehat{x}^{(i)} & = \\frac{x^{(i)}\\;-\\;\\mu_B}{\\sqrt{{\\sigma_B}^2\\;+\\;\\epsilon}} \\\\\n",
    "                           z^{(i)} & = \\gamma\\;\\otimes\\;\\widehat{x}^{(i)}\\;+\\;\\beta\\end{align}$$\n",
    "                           \n",
    "  <paragraph><center>where $\\;\\otimes \\;=\\;$ elementwise$\\;$ multiplication</center></paragraph>\n",
    "  \n",
    "  <paragraph><center>$\\epsilon \\;=\\; $small number to avoid divide-by-zero error, called a smoothing term, typically $10^{-5}$</center></paragraph>\n",
    "  - During backpropagation, each batch-normalized layer learns:\n",
    "    - $\\gamma$ : the output scale vector\n",
    "    - $\\beta$  : the output offset vector\n",
    "    - $\\mu$    : final input mean vector (learned by using a moving average of the layer's input mean)\n",
    "    - $\\sigma$ : final input std dev vector (learned by using a moving average of the layer's input std dev)\n",
    "  - The batch mean/std.dev are used during training, and final mean/std.dev are used after training.\n",
    "  - Benefits:\n",
    "    - Improved neural networks\n",
    "    - Strongly reduced vanishing gradients problem\n",
    "    - Networks were also much less sensitive to weight initialization\n",
    "    - Could use much larger learning rates, speeding up learning\n",
    "    - Acts as a regularizer, reducing the need for anu other regulatization technique\n",
    "    - Does not affect shallower networks as much, but has a tremendous impact on deep networks.\n",
    "    - Although converges faster, training per epoch is rather slow. All in all, wall time will be shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAdd a Batch Normalization layer before/after each hidden layer's activation function,\n",
    "# and optionally after the first layer in your model (as shown here).\n",
    "#\n",
    "# model = Sequential([  # In this tiny example with just 2 hidden layers, it's unlikely that Batch Normalization\n",
    "#                       # will have a very positive impact. For deeper networks, it can make a tremendous difference.\n",
    "#   Flatten(input_shape=[28, 28]),\n",
    "#   BatchNormalization(),\n",
    "#   Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "#   BatchNormalization(),\n",
    "#   Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "#   BatchNormalization(),\n",
    "#   Dense(10, activation='softmax')\n",
    "# ])\n",
    "# model.summary()  # This will show you the Batch Normalization layers. The number of parameters\n",
    "#                  # added for the BN layers is 4 x (number of input parameters).\n",
    "#                  # mu and sigma are not trainable via backprop, and this is what Keras shows at the bottom.\n",
    "# [(var.name, var.trainable) for var in model.layers[1].variables] # will list trainable/untrainable parameters.\n",
    "#\n",
    "# model.layers[1].updates  # Shows the operations Keras created to train the trainable params (gamma, beta)\n",
    "#                          # at each iteration. Since we're using the Tensorflow backend, these are TF operations.\n",
    "\n",
    "# Shows how to add BN layers before the activation function.\n",
    "# You should try adding them before and after the activation function,\n",
    "# and choosing the way that works best.\n",
    "# model = Sequential([\n",
    "#   Flatten(input_shape=[28, 28]),\n",
    "#   BatchNormalization(),\n",
    "#   Dense(300, kernel_initializer='he_normal', use_bias=False), # Removed activation function.\n",
    "#                                                               # Since BN layer already has bias,\n",
    "#                                                               # remove bias from the layer using use_bias=False.\n",
    "#   BatchNormalization(),\n",
    "#   Activation('elu'),\n",
    "#   Dense(100, kernel_initializer='he_normal', use_bias=False),\n",
    "#   BatchNormalization(),\n",
    "#   Activation('elu'),\n",
    "#   Dense(10, activation='softmax')\n",
    "# ])\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\bf Momentum}$ hyperparameter for BN layer:\n",
    "\n",
    "BN layer has a lot of hyperparameters you can tweak.\n",
    "Most defaults will be fine.\n",
    "Usually you would need to tweak the momentum.\n",
    "Given a new vector of input means or std dev (call it $\\widehat{v}\\,$) computed over the current batch,\n",
    "the BN layer updates the running average $\\widehat{v}$ using:\n",
    "$$\\widehat{v} = \\widehat{v}\\;\\times\\;momentum\\;+\\;v\\;\\times\\;(1 - momentum)$$\n",
    "A good momentum is typically close to 1 (ex. 0.9, 0.9, 0.999). You want more 9's for larger datasets, and smaller mini-batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\bf Axis}$ hyperparameter for BN layer:\n",
    "\n",
    "- Determines which axis should be normalized.\n",
    "- Defaults to -1, which means the last axis. So if you have batch shape [batch size, features], the normalization is across the batch size for each feature.\n",
    "- If BN layer was before the Flatten layer, batch shape would be [batch size, height, width], so normalization would be across the batch size and height for each width. This means we would get 28 means and 28 std devs for each column. If you want to use the 784 pixels independently, you should set axis=[1, 2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "\n",
    "- Gradient Clipping is used in Recurrent Neural Networks (RNN), since Batch Normalization is tricky. Other than RNNs, BN layer works fine.\n",
    "- Gradient Clipping clips the gradient during backprop so it does not exceed some threshold. ex:\n",
    "\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)  # all gradients clipped to $\\pm$1.0\n",
    "\n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "- clipvalue can change the orientation of the gradient vector. ex. if original gradient = [0.9, 100.0], after clipping it will be [0.9, 1.0]\n",
    "- use clipnorm instead of clipvalue to keep orientation of gradient vector. This clips gradient if it's L2 norm > threshold. clipnorm=1.0 will change [0.9, 100.0] to [0.00899, 0.9999], preserving it's orientation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pretrained layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transfer learning: \n",
    "  - Use the lower layers of a DNN trained to solve a similar problem as the first layers of your network. ex. To recognize facial expressions, you can use a DNN that is trained to recognize faces. The lower layers of this network will have learned where the mouth, eyes, etc, are on the face. You can then add your upper layers to recognize facial expressions.\n",
    "  - Transfer learning will work best when the inputs have similar low-level features.\n",
    "  - The more similar the tasks are, the more layers you will want to use. For very similar tasks, keep all hidden layers, and replace just the output layer.\n",
    "- Initially, try locking the weights for the pre-trained layers, so when training the upper layers, those weights remain the same.\n",
    "- If that does not work, try unfreezing one or two top hidden layers to let them updated weights during backprop. The more training data you have, the more layers you can unfreeze. Reduce the learning rate when you unfreeze layers.\n",
    "- If you still have problems:\n",
    "  - and you have little training data, try dropping the top hidden layers and freezing all remaining hidden layers.\n",
    "  - and you have plenty of training data, try replacing the top hidden layers instead of dropping them, and even adding more hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you use a model's weights as-is, any weight changes to the new model will affect the original as well.\n",
    "# So you have to clone the original.\n",
    "# This is just and example. Transfer learning does not work well with small dense networks, probably because\n",
    "# - small networks learn few patterns\n",
    "# - dense networks learn very specific patterns which are unlikely to be useful in other tasks\n",
    "# Transfer learning works best with deep convolutional neural networks, which tend to learn\n",
    "# feature detectors that are much more general (especially in the lower layers).\n",
    "# Deep Convolutional networks in Chapter 14\n",
    "# \n",
    "# model_A = keras.models.load_model('my_model_A.h5')\n",
    "# model_A_clone = keras.models.clone_model(model_A) # model_A is the model that solves the similar problem.\n",
    "# model_A_clone.set_weights(model_A.get_weights())  # This is done since clone_model() does not clone weights.\n",
    "# model_B_on_A = Sequential(model_A_clone.layers[:-1]) # Include all layers except output\n",
    "# model_B_on_A.add(Dense(1, activation='sigmoid'))     # Our own output layer\n",
    "\n",
    "# for layer in model_B_on_A.layers[:-1]:               # Freeze borrowed layer weights. This may be needed for the\n",
    "#     layer.trainable = False                          # first few epochs until the new output layer has learned\n",
    "#                                                      # reasonable weights. This is done since it's weights are \n",
    "#                                                      # random and will wreck the borrowed layer weights.\n",
    "\n",
    "# model_B_on_A.compile(loss='binary_crossentropy',     # You must always compile your model after you\n",
    "#                      optimizer='sgd',                # freeze/unfreeze layers.\n",
    "#                      metrics='accuracy')\n",
    "\n",
    "# history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,         # Train the model for a few epochs\n",
    "#                            validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "# for layer in model_B_on_A.layers[:-1]:               # Now our new layer must have learned good weights, so\n",
    "#     layer.trainable = False                          # unfreeze the lower layers.\n",
    "\n",
    "# optimizer = keras.optimizer.SGD(lr=1e-4)             # After unfreezing the lower layers, it is a good idea to\n",
    "#                                                      # lower the learning rate to avoid damaging the\n",
    "#                                                      # lower-layer weights.\n",
    "\n",
    "# model_B_on_A.compile(loss='binary_crossentropy',     # You must always compile your model after you\n",
    "#                      optimizer='sgd',                # freeze/unfreeze layers.\n",
    "#                      metrics='accuracy')\n",
    "\n",
    "# history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "#                            validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised pretraining\n",
    "\n",
    "Suppose you have a complex task but not much labeled training data, and cannot find a model trained on a similar task. You should:\n",
    "  - try to get more labeled training data. If you cannot,\n",
    "  - try to perform unsupervised pretraining:\n",
    "    - Requires plenty of unlabeled training data\n",
    "    - Use it to pretrain an unsupervised model (ex. autoencoder/GAN (Generative Adversarial Network)). See Ch 17.\n",
    "    - Use the lower layers of the unsupervised model, add your output layer on top, and fine-tune the final network using supervised learning (with the labeled training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining on an auxiliary task\n",
    "\n",
    "If you don't have much labeled training data, another approach is to:\n",
    "  - train a first network on an auxiliary task for which you have labeled data. Then reuse the lower layers for your actual task.\n",
    "  - ex. for a Natural Language Processing (NLP) task, you can:\n",
    "    - download a corpus of millions of text documents,\n",
    "    - randomly mask out some words and train a model to predict what those words are.\n",
    "    - This will train your model to \"understand\" the language to some extent.\n",
    "    - Then you can reuse it for your actual task and fine-tune it for your labeled data.\n",
    " \n",
    " Self-supervised learnin:\n",
    "   - you automatically generate the labels from the data itself.\n",
    "   - train a model on the resulting labeled dataset using supervised learning.\n",
    "   - Does not require human labeling whatsoever, so it is best classified as a form of unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most popular algorithms:\n",
    "  - momentum optimization\n",
    "  - Nesterov Accelerated Gradient\n",
    "  - AdaGrad\n",
    "  - RMSProp\n",
    "  - Adam\n",
    "  - Nadam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum Optimization\n",
    "\n",
    "  - Gradient Descent takes small regular steps down the slope. The step size is also a function of the gradient value. If the gradient is tiny, the step size will be tiny also.\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\eta\\nabla_{\\theta}J(\\theta)$$\n",
    "\n",
    "  - Momentum optimization changes the step size as a function of time, quickly taking larger and larger steps.\n",
    "  \n",
    "$$\\begin{align}m & \\leftarrow\\beta m - \\eta \\nabla_{\\theta}J(\\theta) \\\\\n",
    "          \\theta & \\leftarrow \\theta + m \\end{align}$$\n",
    "          <paragraph><center>where $\\beta$ is the momentum and should be set between 0 (high friction) and 1 (no friction).\n",
    "          If $\\beta$ = 0.9, the velocity is 10 x $\\eta\\nabla_{theta}J(\\theta)$, so\n",
    "          momentum optimization goes 10 times faster than gradient descent.</center></paragraph>\n",
    "  - Benefits:\n",
    "    - escapes from plateaus faster than Gradient Descent\n",
    "    - can roll past local optima\n",
    "    - For deep networks that don't use Batch Normalization, the upper layers will end up with inputs of very different scales. Momentum optimization helps a lot to converge for layers across all input scales.\n",
    "  - Issues:\n",
    "    - The optimizer may overshoot in one direction, then overshoot in another thus oscillating for a while before stabilizing at the minimum. Good to have friction to get rid of oscillations and speed up coverage.\n",
    "  - momentum value of 0.9 usually works well in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient (NAG) AKA Nesterov Momentum Optimization\n",
    "\n",
    "  - This variant of momentum optimization is almost always faster than vanilla momentum optimization. In fact it is significantly faster than regular momentum optimization.\n",
    "  - It measures the gradient of the cost function slightly ahead in the direction of the momentum. The vanilla momentum optimization computes the gradient before the momentum update to $\\theta$. NAG updates the momentum to $(\\theta + \\beta \\; m)$ and then applies the gradient to it. This works because in general the momentum will point in the right direction.\n",
    "  \n",
    "  $$\\begin{align}m & \\leftarrow\\beta m - \\eta \\nabla_{\\theta}J(\\theta + \\beta \\; m) \\\\\n",
    "          \\theta & \\leftarrow \\theta + m \\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = kears.optimizer.SGD(lr=0.0001, momentum=0.9, nesterov=True)  # Nesterov Accelerated Gradient (NAG), \n",
    "#                                                                          # AKA Nesterov Momentum Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "  - It would be nice if the algorithm could correct it's direction to point more towards the global optimum. AdaGrad achieves this by scaling down the gradient vector along the steepest dimensions.\n",
    "\n",
    "$$\\begin{align}s & \\leftarrow s + \\nabla_{\\theta}J(\\theta) \\otimes \\nabla_{\\theta} J(\\theta) \\\\\n",
    "         \\theta  & \\leftarrow \\theta - \\eta \\nabla_{\\theta} J(\\theta) \\odiv \\sqrt{s + \\epsilon)} \\end{align}$$\n",
    "         <paragraph><center>Accululate the square of the gradient into the variable s.\n",
    "    Then scale down the gradient vector by a factor of $\\sqrt{s + \\epsilon}$.\n",
    "    The $\\odiv$ is the symbol for elementwise division.\n",
    "    $\\epsilon$ is a smoothing term to avoid div-by-zero. Typically set to 1e-10. This decays the learning rate faster for steeper dimensions than for dimensions with gentler slopes. This is called an adaptive learning rate. </center></paragraph>\n",
    "  - Benefits:\n",
    "    - It helps point the update toward the global optimum.\n",
    "    - This algorithm requires much less tuning of the learning rate $\\eta$\n",
    "    - Performs well for simple quadratic problems - may be efficient for Linear Regression.\n",
    "  - Issues:\n",
    "    - Often stops too early. The learning rate gets scaled down so much that the algorithm stops before reaching global optimum. You should not use it for Deep NNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "\n",
    "  - Since AdaGrad runs the risk of slowing down a bit too fast and never converging, RMSProp accumulates the gradients from only the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in during gradient accumulation (first equation below):\n",
    "\n",
    "$$\\begin{align}s & \\leftarrow \\beta s + (1 - \\beta) \\nabla_{\\theta}J(\\theta) \\otimes \\nabla_{\\theta} J(\\theta) \\\\\n",
    "         \\theta  & \\leftarrow \\theta - \\eta \\nabla_{\\theta} J(\\theta) \\odiv \\sqrt{s + \\epsilon)} \\end{align}$$\n",
    "         <paragraph><center>Accumulate the square of the latest gradients into the variable s.\n",
    "    Then scale down the gradient vector by a factor of $\\sqrt{s + \\epsilon}$.\n",
    "    The $\\odiv$ is the symbol for elementwise division.\n",
    "    The decay rate is $\\beta$.\n",
    "    $\\epsilon$ is a smoothing term to avoid div-by-zero. Typically set to 1e-10. This decays the learning rate faster for steeper dimensions than for dimensions with gentler slopes. This is called an adaptive learning rate. </center></paragraph>\n",
    "\n",
    "  - The decay rate $\\beta$ is typically set to 0.9 (which typically works well).\n",
    "  - Can lead to solutions that generalize poorly on some datasets. So if you're seeing this, try using plain Nesterov Accelerated Gradient instead.\n",
    "  - RMSProp was the preferred optimization algorithm for many researchers until Adam optimization came around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9) # rho is the decay rate beta in the above text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam, AdaMax and Nadam Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Adam (Adaptive Moment Estimation) combines the ideas of momentum optimization and RMSProp.\n",
    "    - It keeps track of exponentially decaying average of past gradients\n",
    "    - Exponentially decaying average of past squared gradients\n",
    "    \n",
    " $$\\begin{align}m & \\leftarrow \\beta_1 m - (1 - \\beta) \\nabla_{\\theta} J(\\theta) \\\\\n",
    "                s & \\leftarrow \\beta_2 s + (1 - \\beta_2) \\nabla_{\\theta}J(\\theta) \\otimes \\nabla_{\\theta} J(\\theta) \\\\\n",
    "      \\widehat{m} & \\leftarrow \\frac{m}{1 - \\beta_1^T} \\\\\n",
    "      \\widehat{s} & \\leftarrow \\frac{s}{1 - \\beta_2^T} \\\\\n",
    "           \\theta & \\leftarrow \\theta + \\;\\eta \\; \\widehat{m} \\; \\odiv \\sqrt{\\widehat{s} + \\epsilon}\n",
    "      \\end{align}$$\n",
    "      <paragraph><center>T represents the iteration number starting at 1.\n",
    "    $\\widehat{m}$ and $\\widehat{s}$ are initialized to 0 at the beginning of training, so the third and fourth equations are needed to boost them.</center></paragraph>\n",
    "    - Momentum decay $\\beta_1$ is typically initialized to 0.9\n",
    "    - Scaling decay $\\beta_2$ is typically initialized to 0.999\n",
    "    - Smoothing term $\\epsilon$ defaults to None, which tells Keras to use keras.backend.epsilon which is 1e-7. If you want, you can change it using keras.backend.set_epsilon()\n",
    "    - Learning rate $\\eta$ is typically set to 0.001. Since Adam is adaptive, it requires less tuning of $\\eta$\n",
    "    - Can lead to solutions that generalize poorly on some datasets. So if you're seeing this, try using plain Nesterov Accelerated Gradient instead.\n",
    "  - AdaMax\n",
    "    - replaces the L2 norm of parameter updates of Adam with the $L_{\\infty}$ norm (the max value of the time-decayed gradients).\n",
    "    - Adam performs better, so you can try this algorithm if you're seeing problems with AdaMax\n",
    "  - Nadam\n",
    "    - This is Adam + Nesterov trick, so it will converge slightly faster than Adam.\n",
    "    - Can lead to solutions that generalize poorly on some datasets. So if you're seeing this, try using plain Nesterov Accelerated Gradient instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Sparse Models\n",
    "\n",
    "- If you need\n",
    "  - blazing fast model at runtime, or\n",
    "  - you need to take up less memory\n",
    "- then\n",
    "  - train the model as usual\n",
    "  - set it's tiny weights to 0 (using model.set_weights()?)\n",
    "  - This may typically not lead to a very sparse model, and\n",
    "  - may degrade the model's performance.\n",
    "- Better option\n",
    "  - Apply strong L1 regularization during training (this zeroes out many weights)\n",
    "- If this is not enough\n",
    "  - Use TensorFlow Model Optimization Toolkit (TF-MOT) which has a pruning API that iteratively removes connections during training based on their magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Comparison\n",
    "\n",
    "<table>\n",
    "    <caption>Optimizer Comparison</caption>\n",
    "    <tr><th>Class</th><th>Convergence Speed</th><th>Convergence quality</th></tr>\n",
    "    <tr><td>SGD</td><td>*</td><td>**</td></tr>\n",
    "    <tr><td>SGD(momentum=...)</td><td>**</td><td>***</td></tr>\n",
    "    <tr><td>SGD(momentum=..., nesterov=True)</td><td>**</td><td>***</td></tr>\n",
    "    <tr><td>Adagrad</td><td>***</td><td>* (stops too early)</td></tr>\n",
    "    <tr><td>RMSProp</td><td>***</td><td>** or ***</td></tr>\n",
    "    <tr><td>Adam</td><td>***</td><td>** or ***</td></tr>\n",
    "    <tr><td>Nadam</td><td>***</td><td>** or ***</td></tr>\n",
    "    <tr><td>AdaMax</td><td>***</td><td>** or ***</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
