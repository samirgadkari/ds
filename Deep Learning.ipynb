{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A large portion of this code is taken from Aur√©lien G√©ron's: Hands-On machine learning with SciKit-Learn, Keras and Tensorflow (2nd edition). I have put comments based on information in the book as well as information I found elsewhere.\n",
    "\n",
    "## Chapter 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding gradients problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanishing gradients:\n",
    "  - If the gradients at the upper layer are small, they're divided and assigned to each neuron in the lower layer.\n",
    "    These values are then further divided and assigned to each neron in the layer below it.\n",
    "    As the number of layers are large in a Deep Neural Network, the gradients at the bottom are zero.\n",
    "    During forward propagation, this causes the weights to remain the same and training never converges.\n",
    "Exploding gradients:\n",
    "  - If the loss function has a steep gradient at the location where you're computing it,\n",
    "    there will be a large change in the weight values. This in turn can cause you to overshoot the minima\n",
    "    and land on the other side of the loss function. If the gradient at that point is also large,\n",
    "    this can cause another large change in the weight values, and another overshoot.\n",
    "  - Sometimes these overshoots will get larger and larger getting you more and more away from the minima.\n",
    "    Sometimes these overshoots will get you into a region of the loss function where the gradients are smaller,\n",
    "    thus temporarily keeping the changes to the weights low. But if you get back down to where the\n",
    "    gradients are large, you may find yourself again overshooting. This will cause you to oscillate,\n",
    "    and never get to the minima.\n",
    "  - More generally, neural networks suffer from unstable gradients. \n",
    "    Different layers may learn at widely different speeds.\n",
    "\n",
    "These problems result from using the Sigmoid activation function, \n",
    "and the Standard Normal initial weight distribution\n",
    "(0 mean, 1 std. dev.). With this initialization, \n",
    "the input variance for each layer increases at the output of that layer. \n",
    "This causes the top layer's sigmoid function to get input that saturates it's output.\n",
    "With a saturated output, the gradient is close to zero.\n",
    "Backpropagation has no gradient to propagate back,\n",
    "any small gradient keeps getting diluted at each lower layer.\n",
    "The Sigmoid function has a mean of 0.5, which exacerbates the problem.\n",
    "The Hyperbolic tangent function behaves better (since it's mean is 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glorot and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution is to use Glorot initialization:\n",
    "$$fan_{avg} = \\frac{fan_{in} + fan_{out}}{2}$$\n",
    "Initialize weights with Normal distribution with 0 mean, variance $\\sigma^2 = \\frac{1}{fan_{avg}}$\n",
    "\n",
    "Or, a uniform distribution between $\\pm$r, with $r = \\sqrt{\\frac{3}{fan_{avg}}} = \\sqrt{3{\\sigma}^2}$\n",
    "\n",
    "Glorot initialization can speed up training considerably.\n",
    "\n",
    "| Initialization | Activation functions | $\\sigma^2$ (Normal) |\n",
    "|---|---|---|\n",
    "|Glorot|None, tanh, logistic, softmax|$\\frac{1}{fan_{avg}}$|\n",
    "|He|ReLU and variants|$\\frac{2}{fan_{in}}$|\n",
    "|LeCun|SELU|$\\frac{1}{fan_{in}}$|\n",
    "\n",
    "By default Keras uses Glorot initialization with a uniform distribution.\n",
    "To use He init with $fan_{in}$:\n",
    "  - keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal') # or kernel_initializer='he_uniform'\n",
    "\n",
    "To use He init with $fan_{avg}$:\n",
    "  - he_avg_init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg', distribution='uniform')\n",
    "  - keras.layers.Dense(10, activation='sigmoid', kernel_initializer=he_avg_init)\n",
    "  - With distribution=\"uniform\", samples are drawn from a uniform distribution within [-limit, limit], with \n",
    "    - limit = sqrt(3 * scale / n).\n",
    "    - n = number of input weights\n",
    "  - So you can specify the limits of the uniform distribution by setting the scale value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default Keras uses Glorot initialization with a uniform distribution. To use He init with ùëìùëéùëõ_ùëñùëõ:\n",
    "\n",
    "# keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal') \n",
    "# or kernel_initializer='he_uniform'\n",
    "\n",
    "# To use He init with ùëìùëéùëõ_ùëéùë£ùëî:\n",
    "# he_avg_init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg', distribution='uniform')\n",
    "# keras.layers.Dense(10, activation='sigmoid', kernel_initializer=he_avg_init)\n",
    "\n",
    "# With distribution=\"uniform\", samples are drawn from a uniform distribution within [-limit, limit], with\n",
    "#     limit = sqrt(3 * scale / n).\n",
    "#     n = number of input weights\n",
    "# So you can specify the limits of the uniform distribution by setting the scale value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU:\n",
    "  - $$\\begin{align}RELU(z) & = 0 \\;\\;\\; if \\; z < 0 \\\\\n",
    "                       & = z \\;\\;\\; if \\; z >= 0 \\end{align}$$                   \n",
    "  - ReLU function thus does not saturate for positive values.\n",
    "  - It is also fast to compute, since we only have to look at\n",
    "    z to decide the output.\n",
    "  - If a layer's weighted sums are negative for all input instances,\n",
    "    the output is 0 (and stays 0 forever). This is the dying ReLU problem.\n",
    "  - Use Leaky ReLU instead\n",
    "  \n",
    "Leaky ReLU:\n",
    "  - $$Leaky \\; ReLU_\\alpha(z) = max(\\alpha z, z)$$\n",
    "  - $\\alpha$ is the slope of the function for negative z\n",
    "  - Setting $\\alpha$ to 0.2 (a huge leak) seems to perform better than 0.01 (a small leak)\n",
    "  - Default value of $\\alpha$ = 0.3\n",
    "  - **Preferred if you're worried about runtime latency**\n",
    "  \n",
    "RReLU (Randomized Leaky ReLU) \n",
    "  - uses random $\\alpha$ for training, \n",
    "    and a fixed average $\\alpha$ for testing.\n",
    "    It performed fairly well and seemed to act as a regularizer\n",
    "  - **Preferred if you're overfitting**\n",
    "  \n",
    "PReLU (Parametric Leaky ReLU) \n",
    "  - allows $\\alpha$ to be learned during training.\n",
    "    It becomes a parameter that backprop can modify.\n",
    "    It strongly outperformed ReLU on large datasets, but risks overfitting on smaller datasets.\n",
    "  - **Preferred if you have a huge training set**\n",
    "  \n",
    "ELU (Exponential Linear Unit):\n",
    "  - Outperformed all ReLUs\n",
    "  - Training time was reduced\n",
    "  - Network performed better on test set\n",
    "  - $$\\begin{align}ELU_\\alpha(z) & = \\alpha(e^z - 1) & if \\; z < 0 \\\\\n",
    "                                 & = z & if \\; z >= 0 \\end{align}$$\n",
    "  - ELU increases linearly with slope 1 for z > 0, and\n",
    "    becomes more and more negative with an asymptote at -$\\alpha$ as z becomes more negative\n",
    "  - If $\\alpha\\,=\\,1$, the function is smooth everywhere, including at z = 0.\n",
    "    This allows Gradient Descent to speed up, as it does not bounce left and right of 0.\n",
    "  - ELU is slower to compute. It's faster training rate helps at training time,\n",
    "    but slow computation hurts during testing.\n",
    "    \n",
    "SELU (Scaled ELU):\n",
    "  - If:\n",
    "    - Your network consists only of a stack on Dense layers\n",
    "    - All hidden layers use the SELU activation function\n",
    "    - Input features are all standardized (mean 0, std. dev. 1)\n",
    "    - All hidden-layer weights are initialized with LeCun normalization (kernel_initializer = 'lecun_normal')\n",
    "    - Network's architecture must be sequential. No Recurrent Networks, Skip connections (Wide and Deep nets).\n",
    "    - Some researchers have mentioned SELU works well for Convolutional Networks as well (even though there are no Dense layers).\n",
    "    \n",
    "**In general, prefer SELU > ELU > Leaky ReLU (and it's variants) > tanh > logistic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you use the ReLU/SELU functions\n",
    "#\n",
    "# model = keras.models.Sequential([\n",
    "#     [ ... ]\n",
    "#     keras.layers.Dense(10, kernel_initializer = \"he_normal\"), \n",
    "#     keras.layers.LeakyReLU(alpha = 0.2),   # For LeakyReLU, this layer should come after each layer you want to\n",
    "#                                            # apply it to. Default alpha = 0.3.\n",
    "#                                            # For PReLU, replace this LeakyReLU() with PReLU().\n",
    "#                                            # No implementation of RReLU() yet - you write your own.\n",
    "#     [ ... ]\n",
    "# ]) \n",
    "\n",
    "# layer = keras.layers.Dense(10, activation='selu', kernel_initializer='lecun_normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization (BN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need it:\n",
    "  - The Glorot/He initialization plus any nonsaturating activation function reduces the danger of vanishing/exploding gradients at the beginning of training. But they can come back during training. BN addresses this issue.\n",
    "\n",
    "How:\n",
    "  - Add BN layer before/after the activation function.\n",
    "\n",
    "How does it work:\n",
    "  - Evaluates mean and std dev of the input over the current mini-batch (hence it's called Batch Normalization).\n",
    "  \n",
    "  $$\\displaystyle\\begin{align}\\mu_B & = \\frac{1}{m_B}\\sum_{i=1}^{m_B}x^{(i)} \\\\\n",
    "                          {\\sigma_B}^2 & = \\frac{1}{m_B}\\sum_{i=1}^{m_B}{(x^{(i)} - \\mu_B)}^2\\end{align}$$\n",
    "  - 0-centers and normalizes each input, then scales and shifts it according to two new parameter vectors per layer.\n",
    "  \n",
    "  $$\\begin{align}\\widehat{x}^{(i)} & = \\frac{x^{(i)}\\;-\\;\\mu_B}{\\sqrt{{\\sigma_B}^2\\;+\\;\\epsilon}} \\\\\n",
    "                           z^{(i)} & = \\gamma\\;\\otimes\\;\\widehat{x}^{(i)}\\;+\\;\\beta\\end{align}$$\n",
    "                           \n",
    "  <paragraph><center>where $\\;\\otimes \\;=\\;$ elementwise$\\;$ multiplication</center></paragraph>\n",
    "  \n",
    "  <paragraph><center>$\\epsilon \\;=\\; $small number to avoid divide-by-zero error, called a smoothing term, typically $10^{-5}$</center></paragraph>\n",
    "  - During backpropagation, each batch-normalized layer learns:\n",
    "    - $\\gamma$ : the output scale vector\n",
    "    - $\\beta$  : the output offset vector\n",
    "    - $\\mu$    : final input mean vector (learned by using a moving average of the layer's input mean)\n",
    "    - $\\sigma$ : final input std dev vector (learned by using a moving average of the layer's input std dev)\n",
    "  - Benefits:\n",
    "    - Improved neural networks\n",
    "    - Strongly reduced vanishing gradients problem\n",
    "    - Networks were also much less sensitive to weight initialization\n",
    "    - Could use much larger learning rates, speeding up learning\n",
    "    - Acts as a regularizer, reducing the need for anu other regulatization technique\n",
    "    - Does not affect shallower networks as much, but has a tremendous impact on deep networks.\n",
    "    - Although converges faster, training per epoch is rather slow. All in all, wall time will be shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAdd a Batch Normalization layer before/after each hidden layer's activation function,\n",
    "# and optionally after the first layer in your model (as shown here).\n",
    "#\n",
    "# model = Sequential([  # In this tiny example with just 2 hidden layers, it's unlikely that Batch Normalization\n",
    "#                       # will have a very positive impact. For deeper networks, it can make a tremendous difference.\n",
    "#   Flatten(input_shape=[28, 28]),\n",
    "#   BatchNormalization(),\n",
    "#   Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "#   BatchNormalization(),\n",
    "#   Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "#   BatchNormalization(),\n",
    "#   Dense(10, activation='softmax')\n",
    "# ])\n",
    "# model.summary()  # This will show you the Batch Normalization layers. The number of parameters\n",
    "#                  # added for the BN layers is 4 x (number of input parameters).\n",
    "#                  # mu and sigma are not trainable via backprop, and this is what Keras shows at the bottom.\n",
    "# [(var.name, var.trainable) for var in model.layers[1].variables] # will list trainable/untrainable parameters.\n",
    "#\n",
    "# model.layers[1].updates  # Shows the operations Keras created to train the trainable params (gamma, beta)\n",
    "#                          # at each iteration. Since we're using the Tensorflow backend, these are TF operations.\n",
    "\n",
    "# Shows how to add BN layers before the activation function.\n",
    "# You should try adding them before and after the activation function,\n",
    "# and choosing the way that works best.\n",
    "# model = Sequential([\n",
    "#   Flatten(input_shape=[28, 28]),\n",
    "#   BatchNormalization(),\n",
    "#   Dense(300, kernel_initializer='he_normal', use_bias=False), # Removed activation function.\n",
    "#                                                               # Since BN layer already has bias,\n",
    "#                                                               # remove bias from the layer using use_bias=False.\n",
    "#   BatchNormalization(),\n",
    "#   Activation('elu'),\n",
    "#   Dense(100, kernel_initializer='he_normal', use_bias=False),\n",
    "#   BatchNormalization(),\n",
    "#   Activation('elu'),\n",
    "#   Dense(10, activation='softmax')\n",
    "# ])\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BN layer has a lot of hyperparameters you can tweak.\n",
    "# Most defaults will be fine.\n",
    "# Usually you would need to tweak the momentum."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
