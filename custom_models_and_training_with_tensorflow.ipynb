{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Chapter 12 of Aurélien Géron's: Hands-On machine learning with SciKit-Learn, Keras and Tensorflow (2nd edition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95% of the code you encounter will require only tf.keras and tf.data. Here you learn to create custom:\n",
    "  - loss functions\n",
    "  - metrics\n",
    "  - layers\n",
    "  - models\n",
    "  - initializers\n",
    "  - regularizers\n",
    "  - weight constraints\n",
    "  - training loop\n",
    "    - 99% of the time you will not need a custom loop\n",
    "    - for the 1% you need it you can apply\n",
    "      - special transformations or constraints to the gradients\n",
    "      - multiple optimizers for different parts of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Library Tour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Well suited for heavy computations\n",
    "  - Fine-tuned for large-scale machine learning\n",
    "  - Developed by the Google Brain team. Powers many of Google's large-scale services\n",
    "  - Overview of Tensorflow offerings:\n",
    "    - Similar to numpy, but with GPU support\n",
    "    - Each operation\n",
    "      - is implemented using highly efficient C++ code\n",
    "      - has multiple implementations called kernels (one for CPUs, one for GPUs, one for TPUs)\n",
    "    - Supports distributed computing\n",
    "    - Extracts computation graph from Python functions, optimizes it, runs it in parallel\n",
    "    - Train a Tensorflow model in one environment (Python on Linux), export the computation graph, run it in another environment (Java on Android)\n",
    "    - Implements autodiff for computing gradients faster\n",
    "    - Provides excellent optimizers like RMSProp and Nadam\n",
    "  - Main uses of Tensorflow:\n",
    "    - High level Deep learning APIs\n",
    "      - tf.keras (recommended)\n",
    "      - tf.estimator\n",
    "    - Low level deep learning APIs\n",
    "      - tf.nn\n",
    "      - tf.losses\n",
    "      - tf.metrics\n",
    "      - tf.optimizers\n",
    "      - tf.train\n",
    "      - tf.initializers\n",
    "    - Autodiff\n",
    "      - tf.GradientTape\n",
    "      - tf.gradients()\n",
    "    - I/O and processing\n",
    "      - tf.data\n",
    "      - tf.feature_column\n",
    "      - tf.audio\n",
    "      - tf.image\n",
    "      - tf.io\n",
    "      - tf.queue\n",
    "    - Visualization with TensorBoard\n",
    "      - tf.summary\n",
    "    - Deployment and optimization\n",
    "      - tf.distribute\n",
    "      - tf.saved_model\n",
    "      - tf.autograph\n",
    "      - tf.graph_util\n",
    "      - tf.lite\n",
    "      - tf.quantization\n",
    "      - tf.tpu\n",
    "      - tf.xla\n",
    "    - Special data structures\n",
    "      - tf.lookup\n",
    "      - tf.nest\n",
    "      - tf.ragged\n",
    "      - tf.sets\n",
    "      - tf.sparse\n",
    "      - tf.strings\n",
    "    - Mathematics, including linear algebra and signal processing\n",
    "      - tf.math\n",
    "      - tf.linalg\n",
    "      - tf.signal\n",
    "      - tf.random\n",
    "      - tf.bitwise\n",
    "    - Miscellaneous\n",
    "      - tf.compat\n",
    "      - tf.config\n",
    "      - and more ...\n",
    "  - TensorFlow library ecosystem\n",
    "    - Tensorboard for visualization\n",
    "    - TensorFlow Extended (TFX) to productionize Tensorflow projects. Includes tools for\n",
    "      - Data validation\n",
    "      - Preprocessing\n",
    "      - Model analysis\n",
    "      - Serving with TF Serving\n",
    "    - To download and re-use pre-trained models for particular datasets:\n",
    "      - https://www.tensorflow.org/resources/models-datasets\n",
    "      - https://github.com/jtoy/awesome-tensorflow\n",
    "      - Tensorflow Hub: https://www.tensorflow.org/hub/\n",
    "      - https://github.com/tensorflow/models/\n",
    "      - Machine learning papers with code: https://paperswithcode.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorflow like numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=86, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])    # matrix\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=87, shape=(), dtype=int32, numpy=10>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(10)                                  # scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2, 3]), tf.float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape, t.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow indexing is same as Numpy indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=91, shape=(2, 1), dtype=float32, numpy=\n",
       "array([[1.],\n",
       "       [4.]], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, :1]                                         # Selecting a column as a 2-D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=95, shape=(2,), dtype=float32, numpy=array([2., 5.], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1]                                         # Selecting a column as a 1-D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=99, shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1, tf.newaxis]                            # The selection is made based on\n",
    "                                                 # t[..., 1] =>\n",
    "                                                 #   Select all dimensions until the last\n",
    "                                                 #   Select only index 1 of the last dimension.\n",
    "                                                 #   Resulting in [2, 5]\n",
    "                                                 # Since tf.newaxis is at the last position,\n",
    "                                                 # the selected values are enclosed inside\n",
    "                                                 # square brackets for each element =>\n",
    "                                                 #   Resulting in [[2], [5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=103, shape=(3, 1), dtype=float32, numpy=\n",
       "array([[1.],\n",
       "       [2.],\n",
       "       [3.]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0, ..., tf.newaxis]                            # The selection is made based on\n",
    "                                                 # t[0, ...] =>\n",
    "                                                 #   Select only index 0 of the first dimension.\n",
    "                                                 #   Resulting in [1, 2, 3]\n",
    "                                                 #   Select all dimensions until the last\n",
    "                                                 # Since tf.newaxis is at the last position,\n",
    "                                                 # the selected values are enclosed inside\n",
    "                                                 # square brackets for each element =>\n",
    "                                                 #   Resulting in [[1], [2], [3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=107, shape=(1, 3), dtype=float32, numpy=array([[1., 2., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0, tf.newaxis, ...]                            # The selection is made based on\n",
    "                                                 # t[0, ...] (tf.newaxis ignored) =>\n",
    "                                                 #   Select only index 0 of the first dimension.\n",
    "                                                 #   Resulting in [1, 2, 3]\n",
    "                                                 #   Select all dimensions until the last\n",
    "                                                 # Since tf.newaxis is at the middle position,\n",
    "                                                 # the selected values are enclosed inside\n",
    "                                                 # square brackets for each element =>\n",
    "                                                 #   Resulting in [[1, 2, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=109, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[11., 12., 13.],\n",
       "       [14., 15., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=110, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=113, shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t @ tf.transpose(t)                              # @ represents matrix multiplication. You can\n",
    "                                                 # also say tf.matmul(t, tf.transpose(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=116, shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(t, tf.transpose(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=121, shape=(3, 2), dtype=float32, numpy=\n",
       "array([[11., 26.],\n",
       "       [14., 35.],\n",
       "       [19., 46.]], dtype=float32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Other math operations:         tf.add(),     tf.multiply(), tf.square(), tf.exp(), tf.sqrt()\n",
    "# Operations available in numpy: tf.reshape(), tf.squeeze(),  tf.tile()\n",
    "# Differently-named operations:  tf.reduce_mean(), tf.reduce_sum(), tf.reduce_max(),\n",
    "#                                tf.math.log()\n",
    "# The names are different from those in numpy, because the operations do different things. ex:\n",
    "# t.T in numpy is the transpose, but it is tf.transpose(t) in Tensorflow.\n",
    "# In numpy, t.T gives you a transposed view on the same data.\n",
    "# In Tensorflow, you get a copy of the transposed data.\n",
    "#\n",
    "# Many classes have aliases. ex. tf.add() is same as tf.math.add().\n",
    "# This helps keep the packages organized while having concise names for common operations.\n",
    "#\n",
    "# If you want code that is usable in other Keras implementations, you should\n",
    "# use only the functions in the keras.backend. However this is only a subset\n",
    "# of all the Tensorflow functions. ex:\n",
    "K = keras.backend\n",
    "K.square(K.transpose(t)) + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors and NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=122, shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([2., 4., 5.])\n",
    "tf.constant(a)                            # Results in a float64 array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=123, shape=(3,), dtype=float32, numpy=array([2., 4., 5.], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy uses float64 by default.\n",
    "# Neural networks (and thus tensorflow) use float32, we should use\n",
    "# tf.constant(a, dtype=tf.float32) instead.\n",
    "tf.constant(a, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy()                                 # Get numpy array from tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow does not do type conversions automatically\n",
    "# tf.constant(2.) + tf.constant(40)         # InvalidArgumentError since float + int\n",
    "# tf.constant(2.) + \\                       # InvalidArgumentError since float32 + float64\n",
    "#   tf.constant(40., dtype=tf.float64)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensors are immutable. They're used to store data.\n",
    "# We cannot use tensors to implement weights in a Neural Network.\n",
    "# We can use Variables instead.\n",
    "# Can be modified in-place using: \n",
    "#   assign() for assigning value to a variable\n",
    "#   assign_add() for incrementing\n",
    "#   assign_sub() for decrementing\n",
    "#\n",
    "# You can also modify individual cells (or slices) using the cells (or slices) assign(),\n",
    "# or by using the scatter_update() or scatter_nd_update(). nd stands for n-dimensions.\n",
    "#\n",
    "# In practice, you will add weights using add_weight() function.\n",
    "# You will rarely need to create variables manually.\n",
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2.,  4.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(2 * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0, 1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  0.],\n",
       "       [ 8., 10.,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[:, 2].assign([0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   0.],\n",
       "       [  8.,  10., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Data Structures\n",
    "  - Sparse tensors (tf.SparseTensor)\n",
    "    - Represent tensors with mostly zeros, efficiently.\n",
    "  - Tensor arrays (tf.TensorArray)\n",
    "    - List of tensors with fixed size by default. Can optionally be made dynamic.\n",
    "      All tensors they contain must have the same shape and data type.\n",
    "  - Ragged tensors (tf.RaggedTensor)\n",
    "    - List of list of tensors, where each tensor has the same shape and data type.\n",
    "  - String tensors\n",
    "    - Regular tensors of type string (represents byte strings). Unicode strings are encoded\n",
    "      to utf-8 automatically. Represent unicode strings using tensors of type tf.int32 with\n",
    "      4 int32 values representing a unicode code point.\n",
    "  - Sets\n",
    "    - Represented as tensors/sparse tensors. ex. tf.constant([[1, 2], [3, 4]]) represents\n",
    "      two sets {1, 2}, and {3, 4}. Each set is represented as a vector in the tensors\n",
    "      last axis.\n",
    "  - Queues\n",
    "    - Store tensors across multiple steps. FIFOQueue, PriorityQueue, RandomShuffleQueue (shuffles it's items), PaddingFIFOQueue (pads it's differently-shaped items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing Models and Training Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your training set is a bit noisy:\n",
    "#   - you remove/fix outliers\n",
    "# but it's still noisy.\n",
    "# With the MSE loss function, it will penalize large errors too much.\n",
    "# With the MAE loss function, it may take a while to converge, or the model may be imprecise.\n",
    "# Use Huber function. It's there in tf.keras.losses.Huber, but we can make one and use it.\n",
    "# def create_huber(threshold=1.0):\n",
    "#   def huber_fn(y_true, y_pred):\n",
    "#     error = y_true - y_pred\n",
    "#     is_small_error = tf.abs(error) < threshold\n",
    "#     squared_loss = tf.square(error) / 2\n",
    "#     linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
    "#     return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "#   return huber_fn\n",
    "\n",
    "# For better performance, use vectorized implementation.\n",
    "# If you want to benefit from TensorFlow's graph features, use only Tensorflow operations.\n",
    "# Return a tensor containing 1 loss per instance, instead of returning mean loss.\n",
    "# This way Keras can apply class weights or sample weights when requested.\n",
    "\n",
    "# model.compile(loss=create_huber(2.0), optimizer='nadam')\n",
    "# model.fit(X_train, y_train, ...)\n",
    "\n",
    "# Keras will use the created huber_fn as the loss function to perform Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading models that contain custom components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras saves the name of the function when you save the model.\n",
    "# But it does not save the threshold value (hyperparameter).\n",
    "# To load it, you have to map the saved function name to the actual function,\n",
    "# and give it the threshold value.\n",
    "# model = keras.models.load_model('model.h5', \n",
    "#                                 custom_objects={'huber_fn': create_huber(2.0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your function does not have any hyperparameters,\n",
    "# then you just need to map the function name to\n",
    "# the function when you load the model.\n",
    "\n",
    "# To get around this issue of saving parameters to functions, \n",
    "# create a subclass and implement it's get_config() method:\n",
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    # Losses are used to calculate a gradient to train the model.\n",
    "    # This is why they must be differentiable everywhere they're evaluated.\n",
    "    # Their gradients should not be 0 everywhere.\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'threshold': self.threshold}\n",
    "\n",
    "# keras.losses.Loss class can be initialized using:\n",
    "#   - name: The name of the loss\n",
    "#   - reduction: Algorithm to use to aggregate individual instance losses.\n",
    "#                Default is 'sum_over_batch_size' which:\n",
    "#                  - weighs samples (if needed)\n",
    "#                  - adds all values\n",
    "#                  - divides by batch size (notice - not by sum_of_weights, \n",
    "#                                           this is not weighted mean)\n",
    "#                Other algorithm options are 'sum' and None\n",
    "\n",
    "# model.compile(HuberLoss(2.0), optimizer='nadam')\n",
    "\n",
    "# When you save the model, Keras calls the loss instance's get_config()\n",
    "# and saves the config as JSON in the HDF5 file.\n",
    "# When you load the model, it calls the from_config() in the HuberLoss() class\n",
    "# (or if not present, in the Loss class). This method creates an instance of\n",
    "# the class, passing the **config to the constructor.\n",
    "\n",
    "# When loading the model, map the class name to the class:\n",
    "# model = keras.models.load_model('model.h5', \n",
    "#                                 custom_objects={'HuberLoss': HuberLoss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Activation functions, Initializers, Regularizers, and Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Keras functionalities such as losses, regularizers, initializers, constraints,\n",
    "# metrics, activation functions, layers, and even full models can be customized in\n",
    "# the same way as above.\n",
    "def my_softplus(z): # return value is just tf.nn.softplus(z)\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "# At each training step, the weights will be passed to the regularization function\n",
    "# to compute the regularization loss, which will be added to the main loss\n",
    "# to get the final training loss.\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "# The constraint function will be called after each training step,\n",
    "# and the layer's weights will be replaced by the constrained weights.\n",
    "def my_positive_weights(weights): # return value is just tf.nn.rely(weights)\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
    "\n",
    "layer = keras.layers.Dense(30, activation=my_softplus,\n",
    "                           kernel_initializer=my_glorot_initializer,\n",
    "                           kernel_regularizer=my_l1_regularizer,\n",
    "                           kernel_constraint=my_positive_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a function has hyperparameters you want to save with the model, \n",
    "# you must subclass the appropriate class and call method:\n",
    "#   - keras.regularizers.Regularizer, implement the __call__() method\n",
    "#   - keras.constraints.Constraint,   implement the __call__() method\n",
    "#   - keras.initializers.Initializer, implement the __call__() method\n",
    "#   - keras.losses.Loss,              implement the call() method\n",
    "#   - keras.layers.Layer (for any layer, including activation functions),\n",
    "#                                     implement the call() method\n",
    "#   - keras.models.Model,             implement the call() method\n",
    "\n",
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {'factor': self.factor}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st batch result: 0.800000011920929\n",
      "2nd batch result: 0.5\n",
      "Final result: 0.5\n"
     ]
    }
   ],
   "source": [
    "# MSE or MAE is usually preferred as a metric,\n",
    "# we will just show here how to use huber as a metric.\n",
    "# model.compile(loss='mse', optimizer='nadam', metrics=[create_huber(2.)])\n",
    "\n",
    "# Keras will compute the metric and keep track of it's mean for each batch.\n",
    "# Most of the time, this is what you want.\n",
    "# This may not be what we want. For example, during classification,\n",
    "# we want to keep track of, say, (true positives / (true positive + false positive)).\n",
    "# Keras will keep track of this ratio, per batch, not over all batches.\n",
    "# keras.metrics.Precision class does this.\n",
    "p = keras.metrics.Precision()\n",
    "print(f'1st batch result: {p([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])}')\n",
    "print(f'2nd batch result: {p([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])}')\n",
    "\n",
    "# This is called a streaming metric (or stateful metric).\n",
    "# It's updated batch-by-batch.\n",
    "print(f'Final result: {p.result()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.variables                   # These track the number of true positives and false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1207, shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.reset_states()              # Reset all counts to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1218, shape=(), dtype=float32, numpy=0.0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1., **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight('total', initializer='zeros')\n",
    "        self.count = self.add_weight('count', initializer='zeros')\n",
    "    \n",
    "    # Updates variables for one batch\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))  # tf.size(x) flattens x\n",
    "                                                                     # and returns it's length\n",
    "    \n",
    "    # Computes final result\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'threshold': self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If tensorflow does not provide a default implementation for a layer,\n",
    "# or if you want to treat a block of layers as a single layer,\n",
    "# you can create a custom layer.\n",
    "\n",
    "# Some layers have no weights, ex.\n",
    "#   - keras.layers.Flatten\n",
    "#   - keras.layers.ReLU\n",
    "# To create such layer, write a function and wrap it in a keras.layers.Lambda.\n",
    "# This layer can be used like any other layer, or as an activation function.\n",
    "exponential_layer = \\\n",
    "    keras.layers.Lambda(lambda x: tf.exp(x)) # or, activation='exp',\n",
    "                                             # or activation=tf.exp,\n",
    "                                             # or activation=keras.activations.exponential\n",
    "# The exponential layer is sometimes used in the output layer of a model\n",
    "# when the values to predict have very different scales (ex. 0.001. 10., 1000.)\n",
    "\n",
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name='kernel', shape=[batch_input_shape[-1], self.units],\n",
    "            initializer='glorot_normal')\n",
    "        self.bias = self.add_weight(\n",
    "            name='bias', shape=[self.units], initializer='zeros')\n",
    "        super().build(batch_input_shape)  # Must be at the end. Sets self.built=True.\n",
    "                                          # This lets Keras know that the layer is built.\n",
    "        \n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    # Usually you can omit the compute_output_shape() method,\n",
    "    # except when the layer is dynamic.\n",
    "    # Returns the shape of the layers outputs.\n",
    "    # In this case it is the same as the first elements of the shape and\n",
    "    # the last element replaced with the number of neurons in the layer.\n",
    "    # In tf.keras, shapes are instances of tf.TensorShape class, which you can\n",
    "    # convert to list using as_list().\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(                   # TensorShape takes a list of dimensions,\n",
    "            batch_input_shape.as_list()[:-1] +   # and creates a shape of a tensor.\n",
    "            [self.units])                        # If list_a = [1, 2, 3],\n",
    "                                                 # list_a + [x, y] = [1, 2, 3, x, y].\n",
    "                                                 # Same as list_a.extend([x, y])\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'units': self.units,\n",
    "                'activation': keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a layer that takes multiple inputs and\n",
    "# has multiple outputs\n",
    "class MyMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):                   # For a layer with multiple inputs (ex. Concatenate),\n",
    "        X1, X2 = X                       # X should be a tuple containing all inputs\n",
    "        return [X1 + X2, X1 * X2, X1 / X2]  # To create a layer with multiple outputs, call()\n",
    "                                            # should return the list of outputs.\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        b1, b2 = batch_input_shape\n",
    "        return [b1, b1, b1]              # should probably handle broadcasting rules.\n",
    "                                         # To create a layer with multiple outputs,\n",
    "                                         # compute_output_shape() should return the \n",
    "                                         # list of batch output shapes (one per output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a layer that handles training and testing differntly (ex. if it has\n",
    "# Dropout or BatchNormalization layers),\n",
    "# add a training argument to call() and decide what to do within call(). ex:\n",
    "class MyGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "        \n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), \n",
    "                                     stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we want to create a model with:\n",
    "#   - Final Dense output layer\n",
    "#   - A Residual Block. Each Residual Block contains\n",
    "#     - A Concatenation layer taking two inputs (one from last Dense layer, \n",
    "#                                                one from first input)\n",
    "#     - A Dense layer\n",
    "#     - A Dense layer\n",
    "#   - A Residual Block X 4\n",
    "#   - A Dense layer\n",
    "#\n",
    "# Inputs flow from bottom to top.\n",
    "# This model is just an example to illustrate how you can build\n",
    "# any model you want with any layer combination.\n",
    "#\n",
    "# This layer contains other layers - it is special.\n",
    "# Keras detects that the hidden attribute contains trackable objects (layers in this case),\n",
    "# so their variables are automatically added to this layer's list of variables.\n",
    "\n",
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.hidden = [keras.layers.Dense(n_neurons, activation='elu',\n",
    "                                          kernel_initializer='he_normal')\n",
    "                       for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z  # This is the concatenation of inputs and output of last Z layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30, activation='elu',\n",
    "                                          kernel_initializer='he_normal')\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs) # first dense layer\n",
    "        for _ in range(4):       # 4 residual blocks\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)       # one more residual block\n",
    "        return self.out(Z)       # final dense output layer\n",
    "\n",
    "# Now you can compile, fit, evaluate this model to make predictions - just like \n",
    "# any other model.\n",
    "# To save model using model.save() and load using keras.models.load_model(),\n",
    "# implement get_config() in both the ResidualBlock class, and ResidualRegressor class.\n",
    "# Or you can save/load weights using save_weights() and load_weights() methods.\n",
    "# Better to use get_config() since you may forget to save/load weights.\n",
    "\n",
    "# Model is a subclass of Layer, so you can use Models as Layers.\n",
    "# Extra functionality in Model are these methods:\n",
    "#   - compile()       - fit()        - evaluate()       - predict()\n",
    "#   - plus a few variants of the above methods\n",
    "#   - get_layers() returns the models layers by name or index\n",
    "#   - save()          - support for keras.models.load_model()\n",
    "#   - support for keras.models.clone_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and Metrics Based on Model Internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier losses were based on labels and predictions \n",
    "# (also some on weights like the L1Regularizer).\n",
    "# You may want to define losses based on other parts\n",
    "# of your model, such as the weights or activations\n",
    "# of it's hidden layers. This may be useful for\n",
    "# regularization or to monitor internal aspects\n",
    "# of the model.\n",
    "# You can compute a custom loss based on any internals\n",
    "# you want, then pass it to model's add_loss() method.\n",
    "\n",
    "# Let's build this model:\n",
    "#   Hidden layer  +  Auxiliary output layer\n",
    "#              Hidden layer\n",
    "#              Hidden layer\n",
    "#              Hidden layer\n",
    "#              Hidden layer\n",
    "#              Hidden layer\n",
    "#   Auxiliary layer = (associated loss = reconstruction loss (MSE(reconstruction - inputs)))\n",
    "#                     By adding the reconstruction loss to the main loss, we will encourage\n",
    "#                     the model to preserve as much information as possible through the\n",
    "#                     hidden layers - even the information not useful to the regression task.\n",
    "#                     In practice, this loss sometimes improves generalization\n",
    "#                     (it is a regularization loss).\n",
    "\n",
    "class ReconstructionRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation='selu',\n",
    "                                          initialization='lecun_normal')\n",
    "                       for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        self.mean_recon_error = keras.metrics.Mean()     # Metric tracks mean recon error\n",
    "\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)  # n_inputs is unknown before build(),\n",
    "                                                         # so this layer is created here.\n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)                 # Scale by 0.05 so reconstruction\n",
    "                                                         # loss does not dominate main loss.\n",
    "        mre_value = self.mean_recon_error(recon_loss)\n",
    "        self.add_metric(mre_value)                       # Add metric value to track the metric\n",
    "        \n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing gradients using Autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
