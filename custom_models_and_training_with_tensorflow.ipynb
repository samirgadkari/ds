{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Chapter 12 of Aurélien Géron's: Hands-On machine learning with SciKit-Learn, Keras and Tensorflow (2nd edition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95% of the code you encounter will require only tf.keras and tf.data. Here you learn to create custom:\n",
    "  - loss functions\n",
    "  - metrics\n",
    "  - layers\n",
    "  - models\n",
    "  - initializers\n",
    "  - regularizers\n",
    "  - weight constraints\n",
    "  - training loop\n",
    "    - 99% of the time you will not need a custom loop\n",
    "    - for the 1% you need it you can apply\n",
    "      - special transformations or constraints to the gradients\n",
    "      - multiple optimizers for different parts of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Library Tour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow in one sentence: TensorFlow is fine-tuned for large-scale machine learning,\n",
    "and it supports parallel and distributed processing.\n",
    "\n",
    "  - Well suited for heavy computations\n",
    "  - Fine-tuned for large-scale machine learning\n",
    "  - Developed by the Google Brain team. Powers many of Google's large-scale services\n",
    "  - Overview of Tensorflow offerings:\n",
    "    - Similar to numpy, but with GPU support\n",
    "    - Each operation\n",
    "      - is implemented using highly efficient C++ code\n",
    "      - has multiple implementations called kernels (one for CPUs, one for GPUs, one for TPUs)\n",
    "    - Supports distributed computing\n",
    "    - Extracts computation graph from Python functions, optimizes it, runs it in parallel\n",
    "    - Train a Tensorflow model in one environment (Python on Linux), export the computation graph, run it in another environment (Java on Android)\n",
    "    - Implements autodiff for computing gradients faster\n",
    "    - Provides excellent optimizers like RMSProp and Nadam\n",
    "  - Main uses of Tensorflow:\n",
    "    - High level Deep learning APIs\n",
    "      - tf.keras (recommended)\n",
    "      - tf.estimator (not recommended - use tf.keras if possible)\n",
    "    - Low level deep learning APIs\n",
    "      - tf.nn\n",
    "      - tf.losses\n",
    "      - tf.metrics\n",
    "      - tf.optimizers\n",
    "      - tf.train\n",
    "      - tf.initializers\n",
    "    - Autodiff\n",
    "      - tf.GradientTape\n",
    "      - tf.gradients()\n",
    "    - I/O and processing\n",
    "      - tf.data\n",
    "      - tf.feature_column\n",
    "      - tf.audio\n",
    "      - tf.image\n",
    "      - tf.io\n",
    "      - tf.queue\n",
    "    - Visualization with TensorBoard\n",
    "      - tf.summary\n",
    "    - Deployment and optimization\n",
    "      - tf.distribute\n",
    "      - tf.saved_model\n",
    "      - tf.autograph\n",
    "      - tf.graph_util\n",
    "      - tf.lite\n",
    "      - tf.quantization\n",
    "      - tf.tpu\n",
    "      - tf.xla\n",
    "    - Special data structures\n",
    "      - tf.lookup\n",
    "      - tf.nest\n",
    "      - tf.ragged\n",
    "      - tf.sets\n",
    "      - tf.sparse\n",
    "      - tf.strings\n",
    "    - Mathematics, including linear algebra and signal processing\n",
    "      - tf.math\n",
    "      - tf.linalg\n",
    "      - tf.signal\n",
    "      - tf.random\n",
    "      - tf.bitwise\n",
    "    - Miscellaneous\n",
    "      - tf.compat\n",
    "      - tf.config\n",
    "      - and more ...\n",
    "  - TensorFlow library ecosystem\n",
    "    - Tensorboard for visualization\n",
    "    - TensorFlow Extended (TFX) to productionize Tensorflow projects. Includes tools for\n",
    "      - Data validation\n",
    "      - Preprocessing\n",
    "      - Model analysis\n",
    "      - Serving with TF Serving\n",
    "    - To download and re-use pre-trained models for particular datasets:\n",
    "      - https://www.tensorflow.org/resources/models-datasets\n",
    "      - https://github.com/jtoy/awesome-tensorflow\n",
    "      - Tensorflow Hub: https://www.tensorflow.org/hub/\n",
    "      - https://github.com/tensorflow/models/\n",
    "      - Machine learning papers with code: https://paperswithcode.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorflow like numpy\n",
    "\n",
    "Main Differences between Tensorflow and Numpy:\n",
    "  - Numpy defaults to 64-bit integers and floats. \n",
    "    Tensorflow defaults to 32-bit integers and floats.\n",
    "    It requires 32-bit floats for neural networks\n",
    "  - Numpy automatically type-casts when it encounters assymetry.\n",
    "    Tensorflow just raises an exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=786, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])    # matrix\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=787, shape=(), dtype=int32, numpy=10>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(10)                                  # scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2, 3]), tf.float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape, t.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow indexing is same as Numpy indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=791, shape=(2, 1), dtype=float32, numpy=\n",
       "array([[1.],\n",
       "       [4.]], dtype=float32)>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, :1]                                         # Selecting a column as a 2-D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=795, shape=(2,), dtype=float32, numpy=array([2., 5.], dtype=float32)>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1]                                         # Selecting a column as a 1-D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=799, shape=(2, 1), dtype=float32, numpy=\n",
       "array([[1.],\n",
       "       [4.]], dtype=float32)>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=803, shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1, tf.newaxis]                            # The selection is made based on\n",
    "                                                 # t[..., 1] =>\n",
    "                                                 #   Select all dimensions until the last\n",
    "                                                 #   Select only index 1 of the last dimension.\n",
    "                                                 #   Resulting in [2, 5]\n",
    "                                                 # Since tf.newaxis is at the last position,\n",
    "                                                 # the selected values are enclosed inside\n",
    "                                                 # square brackets for each element =>\n",
    "                                                 #   Resulting in [[2], [5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=807, shape=(3, 1), dtype=float32, numpy=\n",
       "array([[1.],\n",
       "       [2.],\n",
       "       [3.]], dtype=float32)>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0, ..., tf.newaxis]                            # The selection is made based on\n",
    "                                                 # t[0, ...] =>\n",
    "                                                 #   Select only index 0 of the first dimension.\n",
    "                                                 #   Resulting in [1, 2, 3]\n",
    "                                                 #   Select all dimensions until the last\n",
    "                                                 # Since tf.newaxis is at the last position,\n",
    "                                                 # the selected values are enclosed inside\n",
    "                                                 # square brackets for each element =>\n",
    "                                                 #   Resulting in [[1], [2], [3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=811, shape=(1, 3), dtype=float32, numpy=array([[1., 2., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0, tf.newaxis, ...]                            # The selection is made based on\n",
    "                                                 # t[0, ...] (tf.newaxis ignored) =>\n",
    "                                                 #   Select only index 0 of the first dimension.\n",
    "                                                 #   Resulting in [1, 2, 3]\n",
    "                                                 #   Select all dimensions until the last\n",
    "                                                 # Since tf.newaxis is at the middle position,\n",
    "                                                 # the selected values are enclosed inside\n",
    "                                                 # square brackets for each element =>\n",
    "                                                 #   Resulting in [[1, 2, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=813, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[11., 12., 13.],\n",
       "       [14., 15., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=814, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=817, shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t @ tf.transpose(t)                              # @ represents matrix multiplication. You can\n",
    "                                                 # also say tf.matmul(t, tf.transpose(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=820, shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(t, tf.transpose(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=825, shape=(3, 2), dtype=float32, numpy=\n",
       "array([[11., 26.],\n",
       "       [14., 35.],\n",
       "       [19., 46.]], dtype=float32)>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Other math operations:         tf.add(),     tf.multiply(), tf.square(), tf.exp(), tf.sqrt()\n",
    "# Operations available in numpy: tf.reshape(), tf.squeeze(),  tf.tile()\n",
    "# Differently-named operations:  tf.reduce_mean(), tf.reduce_sum(), tf.reduce_max(),\n",
    "#                                tf.math.log()\n",
    "# The names are different from those in numpy, because the operations do different things. ex:\n",
    "# t.T in numpy is the transpose, but it is tf.transpose(t) in Tensorflow.\n",
    "# In numpy, t.T gives you a transposed view on the same data.\n",
    "# In Tensorflow, you get a copy of the transposed data.\n",
    "#\n",
    "# Many classes have aliases. ex. tf.add() is same as tf.math.add().\n",
    "# This helps keep the packages organized while having concise names for common operations.\n",
    "#\n",
    "# If you want code that is usable in other Keras implementations, you should\n",
    "# use only the functions in the keras.backend. However this is only a subset\n",
    "# of all the Tensorflow functions. ex:\n",
    "K = keras.backend\n",
    "K.square(K.transpose(t)) + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors and NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=826, shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([2., 4., 5.])\n",
    "tf.constant(a)                            # Results in a float64 array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 4., 5.]), (3,))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=827, shape=(3,), dtype=float32, numpy=array([2., 4., 5.], dtype=float32)>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy uses float64 by default.\n",
    "# Neural networks (and thus tensorflow) use float32, we should use\n",
    "# tf.constant(a, dtype=tf.float32) instead.\n",
    "tf.constant(a, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy()                                 # Get numpy array from tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same outputs from both these statements, except for dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=777, shape=(10,), dtype=int64, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(np.arange(10))                # The output of this statement is the same as\n",
    "                                          # the output of tf.constant(np.arange(10)),\n",
    "                                          # except for the dtype !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=776, shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(10)                              # The output of this statement is the same as\n",
    "                                          # the output of tf.range(10),\n",
    "                                          # except for the dtype !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=778, shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(np.arange(10, dtype='int32')) # Now the output is the same as tf.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=785, shape=(10,), dtype=int64, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(10, dtype='int64')               # Now the output is the same as \n",
    "                                          # tf.constant(np.arange(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow does not do type conversions automatically\n",
    "# tf.constant(2.) + tf.constant(40)         # InvalidArgumentError since float + int\n",
    "# tf.constant(2.) + \\                       # InvalidArgumentError since float32 + float64\n",
    "#   tf.constant(40., dtype=tf.float64)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensors are immutable. They're used to store data.\n",
    "# We cannot use tensors to implement weights in a Neural Network.\n",
    "# We can use Variables instead.\n",
    "# Variables can be modified in-place using: \n",
    "#   assign() for assigning value to a variable\n",
    "#   assign_add() for incrementing\n",
    "#   assign_sub() for decrementing\n",
    "#\n",
    "# You can also modify individual cells (or slices) using the cells (or slices) assign(),\n",
    "# or by using the scatter_update() or scatter_nd_update(). nd stands for n-dimensions.\n",
    "#\n",
    "# In practice, you will add weights using add_weight() function.\n",
    "# You will rarely need to create variables manually.\n",
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2.,  4.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(2 * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0, 1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  0.],\n",
       "       [ 8., 10.,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[:, 2].assign([0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   0.],\n",
       "       [  8.,  10., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Data Structures\n",
    "  - Sparse tensors (tf.SparseTensor)\n",
    "    - Represent tensors with mostly zeros, efficiently.\n",
    "  - Tensor arrays (tf.TensorArray)\n",
    "    - List of tensors with fixed size by default. Can optionally be made dynamic.\n",
    "      All tensors they contain must have the same shape and data type.\n",
    "  - Ragged tensors (tf.RaggedTensor)\n",
    "    - List of tensors, where each tensor can have different length. ex:\n",
    "      rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []]\n",
    "  - String tensors\n",
    "    - Regular tensors of type string (represents byte strings). Unicode strings are encoded\n",
    "      to utf-8 automatically. Represent unicode strings using tensors of type tf.int32 with\n",
    "      4 int32 values representing a unicode code point.\n",
    "  - Sets\n",
    "    - Represented as tensors/sparse tensors. ex. tf.constant([[1, 2], [3, 4]]) represents\n",
    "      two sets {1, 2}, and {3, 4}. Each set is represented as a vector in the tensors\n",
    "      last axis.\n",
    "  - Queues\n",
    "    - Store tensors across multiple steps. FIFOQueue, PriorityQueue, RandomShuffleQueue (shuffles it's items), PaddingFIFOQueue (pads it's differently-shaped items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing Models and Training Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your training set is a bit noisy:\n",
    "#   - you remove/fix outliers\n",
    "# but it's still noisy.\n",
    "# With the MSE loss function, it will penalize large errors too much.\n",
    "# With the MAE loss function, it may take a while to converge, or the model may be imprecise.\n",
    "# Use Huber function. It's there in tf.keras.losses.Huber, but we can make one and use it.\n",
    "# def create_huber(threshold=1.0):\n",
    "#   def huber_fn(y_true, y_pred):\n",
    "#     error = y_true - y_pred\n",
    "#     is_small_error = tf.abs(error) < threshold\n",
    "#     squared_loss = tf.square(error) / 2\n",
    "#     linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
    "#     return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "#   return huber_fn\n",
    "\n",
    "# For better performance, use vectorized implementation.\n",
    "# If you want to benefit from TensorFlow's graph features, use only Tensorflow operations.\n",
    "# Return a tensor containing 1 loss per instance, instead of returning mean loss.\n",
    "# This way Keras can apply class weights or sample weights when requested.\n",
    "\n",
    "# model.compile(loss=create_huber(2.0), optimizer='nadam')\n",
    "# model.fit(X_train, y_train, ...)\n",
    "\n",
    "# Keras will use the created huber_fn as the loss function to perform Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading models that contain custom components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras saves the name of the function when you save the model.\n",
    "# But it does not save the threshold value (hyperparameter).\n",
    "# To load it, you have to map the saved function name to the actual function,\n",
    "# and give it the threshold value.\n",
    "# model = keras.models.load_model('model.h5', \n",
    "#                                 custom_objects={'huber_fn': create_huber(2.0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should you use a function for your loss, or a class?\n",
    "# If your function does not have any hyperparameters,\n",
    "# then you just need to map the function name to\n",
    "# the function when you load the model.\n",
    "# If your function has hyperparameters, it's best to\n",
    "#   - subclass from keras.losses.Loss,\n",
    "#   - pass in the hyperparameter into the __init__() and save it\n",
    "#   - use the hyperparameter in the call() method\n",
    "#   - and provide a get_config() method to return the full config\n",
    "#     - the config for the keras.losses.Loss and\n",
    "#     - the hyperparameter for our class\n",
    "#   - when loading the model from the file, map the class name to the class\n",
    "\n",
    "# To get around this issue of saving parameters to functions, \n",
    "# create a subclass and implement it's get_config() method:\n",
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    # Losses are used to calculate a gradient to train the model.\n",
    "    # This is why they must be differentiable everywhere they're evaluated.\n",
    "    # Their gradients should not be 0 everywhere.\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'threshold': self.threshold}\n",
    "\n",
    "# keras.losses.Loss class can be initialized using:\n",
    "#   - name: The name of the loss\n",
    "#   - reduction: Algorithm to use to aggregate individual instance losses.\n",
    "#                Default is 'sum_over_batch_size' which:\n",
    "#                  - weighs samples (if needed)\n",
    "#                  - adds all values\n",
    "#                  - divides by batch size (notice - not by sum_of_weights, \n",
    "#                                           this is not weighted mean)\n",
    "#                Other algorithm options are 'sum' and None\n",
    "\n",
    "# model.compile(HuberLoss(2.0), optimizer='nadam')\n",
    "\n",
    "# When you save the model, Keras calls the loss instance's get_config()\n",
    "# and saves the config as JSON in the HDF5 file.\n",
    "# When you load the model, it calls the from_config() in the HuberLoss() class\n",
    "# (or if not present, in the Loss class). This method creates an instance of\n",
    "# the class, passing the **config to the constructor.\n",
    "\n",
    "# When loading the model, map the class name to the class:\n",
    "# model = keras.models.load_model('model.h5', \n",
    "#                                 custom_objects={'HuberLoss': HuberLoss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Activation functions, Initializers, Regularizers, and Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Keras functionalities such as losses, regularizers, initializers, constraints,\n",
    "# metrics, activation functions, layers, and even full models can be customized in\n",
    "# the same way as above.\n",
    "def my_softplus(z): # return value is just tf.nn.softplus(z)\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "# At each training step, the weights will be passed to the regularization function\n",
    "# to compute the regularization loss, which will be added to the main loss\n",
    "# to get the final training loss.\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_l2_regularizer(weights):                   # We divide by 2. since the derivative\n",
    "    return tf.reduce_sum(tf.square(weights)) / 2. # of x^2 = 2 x, and this removes the 2\n",
    "                                                  # when finding gradient of the loss function\n",
    "\n",
    "# The constraint function will be called after each training step,\n",
    "# and the layer's weights will be replaced by the constrained weights.\n",
    "def my_positive_weights(weights): # return value is just tf.nn.relu(weights)\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
    "\n",
    "layer = keras.layers.Dense(30, activation=my_softplus,\n",
    "                           kernel_initializer=my_glorot_initializer,\n",
    "                           kernel_regularizer=my_l1_regularizer,\n",
    "                           kernel_constraint=my_positive_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a function has hyperparameters you want to save with the model, \n",
    "# you must subclass the appropriate class and implement this call method:\n",
    "#   - keras.regularizers.Regularizer, implement the __call__() method\n",
    "#   - keras.constraints.Constraint,   implement the __call__() method\n",
    "#   - keras.initializers.Initializer, implement the __call__() method\n",
    "#   - keras.losses.Loss,              implement the call() method\n",
    "#   - keras.layers.Layer (for any layer, including activation functions),\n",
    "#                                     implement the call() method\n",
    "#   - keras.models.Model,             implement the call() method\n",
    "\n",
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {'factor': self.factor}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st batch result: 0.800000011920929\n",
      "2nd batch result: 0.5\n",
      "Final result: 0.5\n"
     ]
    }
   ],
   "source": [
    "# MSE or MAE is usually preferred as a metric,\n",
    "# we will just show here how to use huber as a metric.\n",
    "# model.compile(loss='mse', optimizer='nadam', metrics=[create_huber(2.)])\n",
    "\n",
    "# Keras will compute the metric and keep track of it's mean for each batch.\n",
    "# Most of the time, this is what you want.\n",
    "# This may not be what we want. For example, during classification,\n",
    "# we want to keep track of, say, (true positives / (true positive + false positive)).\n",
    "# Keras will keep track of this ratio, per batch, not over all batches.\n",
    "# keras.metrics.Precision class does this.\n",
    "p = keras.metrics.Precision()\n",
    "print(f'1st batch result: {p([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])}')\n",
    "print(f'2nd batch result: {p([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])}')\n",
    "\n",
    "# This is called a streaming metric (or stateful metric).\n",
    "# It's updated batch-by-batch.\n",
    "print(f'Final result: {p.result()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.variables                   # These track the number of true positives and false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=988, shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.reset_states()              # Reset all counts to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=999, shape=(), dtype=float32, numpy=0.0>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When would you use a function for a custom metric\n",
    "# versus a class?\n",
    "# If you have a streaming metric, then you would need a class,\n",
    "# since you need to keep track of parameters, and use those\n",
    "# parameters in the update_state() function to calculate\n",
    "# your metric. Also, if you have hyperparameters for your metric,\n",
    "# you should use a class.\n",
    "# If you don't need a streaming metric/hyperparameters,\n",
    "# then you can use a function.\n",
    "# In the custom metric class:\n",
    "#   - init():         Pass in the hyperparameter, and save it.\n",
    "#                     Save parameters by using \n",
    "#                       self.param = self.add_weight('name', initializer=...)\n",
    "#   - update_state(): Get the metric by metric = cutom_fn(y_true, y_pred)\n",
    "#                     Update your parameters\n",
    "#   - result():       Return the result by calculating using the parameters\n",
    "#   - get_config():   return the super.get_config() and your class's hyperparameter,\n",
    "#                     as a key-value pair\n",
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1., **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight('total', initializer='zeros')\n",
    "        self.count = self.add_weight('count', initializer='zeros')\n",
    "    \n",
    "    # Updates variables for one batch\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))  # tf.size(x) flattens x\n",
    "                                                                     # and returns it's length\n",
    "        # Here we ignore sample_weight, but to use it, we would do this?:\n",
    "        # self.add_weight('sample_weights', \n",
    "        #                 shape=sample_weights.shape, \n",
    "        #                 initializer=sample_weights)\n",
    "    \n",
    "    # Computes final result\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'threshold': self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should you create a function for a custom layer,\n",
    "# or a class?\n",
    "# If you have a custom layer that has no weights,\n",
    "# like the exponential_layer defined below,\n",
    "# create a function.\n",
    "# If you have weights for your layer, you should create a class:\n",
    "#   - init():   pass in the activation function and save it\n",
    "#   - build():  pass in the batch_input_shape\n",
    "#               create kernel by adding weights and initializer\n",
    "#               create bias by adding weights and the initializer\n",
    "#               super.build() as the last call\n",
    "#   - call():   return self.activation(X @ self.kernel + self.bias)\n",
    "#   - get_config(): return the super.get_config() and your class's hyperparameter,\n",
    "#                   as a key-value pair\n",
    "\n",
    "# If tensorflow does not provide a default implementation for a layer,\n",
    "# or if you want to treat a block of layers as a single layer,\n",
    "# you can create a custom layer.\n",
    "\n",
    "# Some layers have no weights, ex.\n",
    "#   - keras.layers.Flatten\n",
    "#   - keras.layers.ReLU\n",
    "# To create such layer, write a function and wrap it in a keras.layers.Lambda.\n",
    "# This layer can be used like any other layer, or as an activation function.\n",
    "exponential_layer = \\\n",
    "    keras.layers.Lambda(lambda x: tf.exp(x)) # or, activation='exp',\n",
    "                                             # or activation=tf.exp,\n",
    "                                             # or activation=keras.activations.exponential\n",
    "# The exponential layer is sometimes used in the output layer of a model\n",
    "# when the values to predict have very different scales (ex. 0.001, 10., 1000.)\n",
    "\n",
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name='kernel', shape=[batch_input_shape[-1], self.units],\n",
    "            initializer='glorot_normal')\n",
    "        self.bias = self.add_weight(\n",
    "            name='bias', shape=[self.units], initializer='zeros')\n",
    "        super().build(batch_input_shape)  # Must be at the end. Sets self.built=True.\n",
    "                                          # This lets Keras know that the layer is built.\n",
    "        \n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    # Usually you can omit the compute_output_shape() method,\n",
    "    # except when the layer is dynamic.\n",
    "    # compute_output_shape() returns the shape of the layers outputs.\n",
    "    # In this case it is the same as the first elements of the shape and\n",
    "    # the last element replaced with the number of neurons in the layer.\n",
    "    # In tf.keras, shapes are instances of tf.TensorShape class, which you can\n",
    "    # convert to list using as_list().\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(                   # TensorShape takes a list of dimensions,\n",
    "            batch_input_shape.as_list()[:-1] +   # and creates a shape of a tensor.\n",
    "            [self.units])                        # If list_a = [1, 2, 3],\n",
    "                                                 # list_a + [x, y] = [1, 2, 3, x, y].\n",
    "                                                 # Same as list_a.extend([x, y])\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'units': self.units,\n",
    "                'activation': keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a layer that takes multiple inputs and\n",
    "# has multiple outputs\n",
    "class MyMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):                   # For a layer with multiple inputs (ex. Concatenate),\n",
    "        X1, X2 = X                       # X should be a tuple containing all inputs\n",
    "        return [X1 + X2, X1 * X2, X1 / X2]  # To create a layer with multiple outputs, call()\n",
    "                                            # should return the list of outputs.\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        b1, b2 = batch_input_shape\n",
    "        return [b1, b1, b1]              # should probably handle broadcasting rules.\n",
    "                                         # To create a layer with multiple outputs,\n",
    "                                         # compute_output_shape() should return the \n",
    "                                         # list of batch output shapes (one per output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a layer that handles training and testing differntly (ex. if it has\n",
    "# Dropout or BatchNormalization layers),\n",
    "# add a training argument to call() and decide what to do within call(). ex:\n",
    "class MyGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "        \n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), \n",
    "                                     stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we want to create a model with:\n",
    "#   - Final Dense output layer\n",
    "#   - A Residual Block. Each Residual Block contains\n",
    "#     - A Concatenation layer taking two inputs (one from last Dense layer, \n",
    "#                                                one from first input)\n",
    "#     - A Dense layer\n",
    "#     - A Dense layer\n",
    "#   - A Residual Block X 4\n",
    "#   - A Dense layer\n",
    "#\n",
    "# Inputs flow from bottom to top.\n",
    "# This model is just an example to illustrate how you can build\n",
    "# any model you want with any layer combination.\n",
    "#\n",
    "# This layer contains other layers - it is special.\n",
    "# Keras detects that the hidden attribute contains trackable objects (layers in this case),\n",
    "# so their variables are automatically added to this layer's list of variables.\n",
    "\n",
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.hidden = [keras.layers.Dense(n_neurons, activation='elu',\n",
    "                                          kernel_initializer='he_normal')\n",
    "                       for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z  # This is the concatenation of inputs and output of last Z layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30, activation='elu',\n",
    "                                          kernel_initializer='he_normal')\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs) # first dense layer\n",
    "        for _ in range(4):       # 4 residual blocks\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)       # one more residual block\n",
    "        return self.out(Z)       # final dense output layer\n",
    "\n",
    "# Now you can compile, fit, evaluate this model to make predictions - just like \n",
    "# any other model.\n",
    "# To save model using model.save() and load using keras.models.load_model(),\n",
    "# implement get_config() in both the ResidualBlock class, and ResidualRegressor class.\n",
    "# Or you can save/load weights using save_weights() and load_weights() methods.\n",
    "# Better to use get_config() since you may forget to save/load weights.\n",
    "\n",
    "# Model is a subclass of Layer, so you can use Models as Layers.\n",
    "# Extra functionality in Model are these methods:\n",
    "#   - compile()       - fit()        - evaluate()       - predict()\n",
    "#   - plus a few variants of the above methods\n",
    "#   - get_layers() returns the models layers by name or index\n",
    "#   - save()          - support for keras.models.load_model()\n",
    "#   - support for keras.models.clone_model()\n",
    "\n",
    "# When would you need to create a dynamic Keras model?\n",
    "# How do you do that?\n",
    "# Why not make all your models dynamic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and Metrics Based on Model Internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier losses were based on labels and predictions \n",
    "# (also some on weights like the L1Regularizer).\n",
    "# You may want to define losses based on other parts\n",
    "# of your model, such as the weights or activations\n",
    "# of it's hidden layers. This may be useful for\n",
    "# regularization or to monitor internal aspects\n",
    "# of the model.\n",
    "# You can compute a custom loss based on any internals\n",
    "# you want, then pass it to model's add_loss() method.\n",
    "\n",
    "# Let's build this model:\n",
    "#   Hidden layer  +  Auxiliary output layer\n",
    "#              Hidden layer\n",
    "#              Hidden layer\n",
    "#              Hidden layer\n",
    "#              Hidden layer\n",
    "#              Hidden layer\n",
    "#   Auxiliary layer = (associated loss = reconstruction loss (MSE(reconstruction - inputs)))\n",
    "#                     By adding the reconstruction loss to the main loss, we will encourage\n",
    "#                     the model to preserve as much information as possible through the\n",
    "#                     hidden layers - even the information not useful to the regression task.\n",
    "#                     In practice, this loss sometimes improves generalization\n",
    "#                     (it is a regularization loss).\n",
    "\n",
    "class ReconstructionRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation='selu',\n",
    "                                          initialization='lecun_normal')\n",
    "                       for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        self.mean_recon_error = keras.metrics.Mean()     # Metric tracks mean recon error\n",
    "\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)  # n_inputs is unknown before build(),\n",
    "                                                         # so this layer is created here.\n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)                 # Scale by 0.05 so reconstruction\n",
    "                                                         # loss does not dominate main loss.\n",
    "        mre_value = self.mean_recon_error(recon_loss)\n",
    "        self.add_metric(mre_value)                       # Add metric value to track the metric\n",
    "        \n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing gradients using Autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating gradients using differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.000003007075065"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To understand how to use autodiff, let's consider differentiating this function:\n",
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2\n",
    "\n",
    "# You can differentiate this analytically by finding the partial derivatives\n",
    "# with respect to w1, and w2.\n",
    "# The function for a neural network would be much more complex,\n",
    "# with tens of thousands of parameters, and finding the derivative\n",
    "# by hand will be impossible. So you could find an approximation:\n",
    "w1, w2 = 5, 3\n",
    "eps = 1e-6\n",
    "(f(w1 + eps, w2) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.000000003174137"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f(w1, w2 + eps) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating gradients using GradientTape (Autodiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=248, shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: id=240, shape=(), dtype=float32, numpy=10.0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are minor errors in these derivative values.\n",
    "# Additionally, to calculate these you need to call the function twice.\n",
    "# In general you need to call the function as many times as there are parameters.\n",
    "# Evaluating this at each point will take a long time.\n",
    "# Autodiff allows us to find the answer much much faster.\n",
    "\n",
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:           # GradientTape records each operation that involves\n",
    "    z = f(w1, w2)                         # a variable. To save memory, only put what is\n",
    "                                          # required within the tf.GradientTape() block.\n",
    "                                          # Alternatively, pause recording using a\n",
    "                                          #   with tape.stop_recording():\n",
    "                                          # block inside the tf.GradientTape() block.\n",
    "gradients = tape.gradient(z, (w1, w2))    # The tape is automatically erased immediately after\n",
    "gradients                                 # calling it's gradient() method. Calling gradient()\n",
    "                                          # again will cause a runtime error.\n",
    "\n",
    "# These derivatives have no error in them.\n",
    "# The gradient() method only goes through the recorded computations once (in reverse order).\n",
    "# So no matter the number of variables, this method is incredibly efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the tape persistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To call gradient() more than once, make the tape persistent,\n",
    "# and delete it each time you're done with it to free resources.\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)             # results in tensor of value 36.0\n",
    "dz_dw2 = tape.gradient(z, w2)             # results in tensor of value 10.0\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forcing the tape to watch any tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By default, the tape will only track variables.\n",
    "# If you try to compute the gradient of z w.r.t.\n",
    "# anything else, the result will be None\n",
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "tape.gradient(z, [c1, c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=315, shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: id=307, shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can force the tape to watch any tensors:\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "tape.gradient(z, [c1, c2])\n",
    "\n",
    "# If your inputs vary slightly, but your activations vary a lot,\n",
    "# you can create a regularization loss based on the gradient\n",
    "# of these activations w.r.t. the inputs.\n",
    "# Since the inputs are constant values, you can use\n",
    "# tape.watch(inputs) and find the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient of a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you try to compute the gradient of a vector containing multiple losses,\n",
    "# Tensorflow will calculate the gradient of the vector's sum.\n",
    "# If you need gradients for each component of the vector w.r.t. the model parameters,\n",
    "# you must call the tape's jacobian()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop gradients from backpropagating through some part of your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=394, shape=(), dtype=float32, numpy=30.0>, None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To stop gradients from backpropagating through some part of your network,\n",
    "# you must use tf.stop_gradient().\n",
    "# tf.stop_gradient() acts as an identity function during the forward pass,\n",
    "# but it does not allow gradients to flow back (it acts like a constant).\n",
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)           # same result as without stop_gradient() on foward pass\n",
    "\n",
    "tape.gradient(z, (w1, w2))  # returns (tensor 30, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing errors when calculating gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=410, shape=(), dtype=float32, numpy=nan>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sometimes, when computing gradients, we get a result of NaN. ex.\n",
    "# Computing the gradients of my_softplus() function for large inputs.\n",
    "x = tf.Variable(100.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "\n",
    "tape.gradient(z, [x])  # retrns NaN result. This is because of floating point precision errors,\n",
    "                       # autodiff computed inf/inf which results in NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor: id=721, shape=(), dtype=float32, numpy=0.990099>],\n",
       " <tf.Tensor: id=714, shape=(), dtype=float32, numpy=100.0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We know the derivative of the softplus function is 1 / (1 + 1/exp(x)) which is\n",
    "# numerically stable. To use this stable function when computing the derivative of\n",
    "# my_softplus():\n",
    "#   - decorate my_softplus() with @tf.custom_gradient making it return it's normal output +\n",
    "#     the function that computes it's derivatives. The numerically stable derivative fn,\n",
    "#     will receive as input the gradients propagated so far. According to the chain rule,\n",
    "#     we should multiply those gradients with this derivative fn.\n",
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    \n",
    "    # The main output still explodes \n",
    "    # because of the exponential.\n",
    "    # You can use tf.where() to return the inputs \n",
    "    # when they're large instead.\n",
    "    exp = tf.where(z > 30., z, tf.math.log(tf.exp(z) + 1.))\n",
    "    \n",
    "    def my_softplus_gradients(grad):\n",
    "        return grad / (1 + 1 / exp)\n",
    "    return exp, my_softplus_gradients\n",
    "\n",
    "# Now when we calculate the gradients for large input values,\n",
    "# we will get the correct result.\n",
    "x = tf.Variable(100.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_better_softplus(x)\n",
    "\n",
    "tape.gradient(z, [x]), z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes, the fit() method may not be flexible enough for what you need to do.\n",
    "# ex. The Wide and Deep paper uses one optimizer for the wide path, and one for the deep.\n",
    "# Since fit() uses only one optimizer (the one specified when compiling the model),\n",
    "# implementing this paper requires writing your own custom loop.\n",
    "# Unless you really need the flexibility, stick with the fit() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='elu', kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "])\n",
    "\n",
    "def random_batch(X, y, batch_size=32):                # Function that samples a batch of\n",
    "    idx = np.random.randint(len(X), size=batch_size)  # instances from the training set.\n",
    "    return X[idx], y[idx]                             # The Data API is much better.\n",
    "                                                      # Use it instead.\n",
    "\n",
    "# Displays the training status including:\n",
    "#   - number of steps\n",
    "#   - total number of steps\n",
    "#   - mean loss since start of epoch\n",
    "# Use the tqdm library to show a progress bar.\n",
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "                         for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics,\n",
    "          end=end)\n",
    "    \n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]\n",
    "\n",
    "# This training loop does not handle layers that behave differently\n",
    "# between training and testing (ex. BatchNormalization, Dropout).\n",
    "# To handle these, call the model with training=True and make sure\n",
    "# it propagates this to every layer that needs it.\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            \n",
    "            # The mean_squared_error loss function returns one loss per instance\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred)) # You can weigh each error\n",
    "                                                                 # here before calculating\n",
    "                                                                 # the mean.\n",
    "    \n",
    "            # The + operator does broadcasting, \n",
    "            # so here each element of the [main_loss] array will get \n",
    "            # model_loss added to it.\n",
    "            loss = tf.add_n([main_loss] + model.losses)     # tf.add_n() waits for all inputs \n",
    "                                                            # to be ready, then accumulates \n",
    "                                                            # (sums) the inputs together.\n",
    "                                                            # It sums multiple tensors of\n",
    "                                                            # the same shape and data type.\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        # You can apply any transformation to the gradients before the apply_gradients()\n",
    "        # function call.\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        # If you add weight constraints or bias constraints when creating a layer,\n",
    "        # this will apply the constraints. This needs to be just after apply_gradients()\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "                \n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Functions and Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=761, shape=(), dtype=int32, numpy=8>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cube(x):\n",
    "    return x ** 3\n",
    "\n",
    "tf_cube = tf.function(cube)  # Converts this Python function to a Tensorflow function\n",
    "tf_cube(2)                   # Now it returns the same values, but as Tensors\n",
    "\n",
    "# Under the hood, tf.function() analyzed the computations\n",
    "# performed and generated a computation graph.\n",
    "# Tensorflow optimizes the compuation graph.\n",
    "# During execution, the TF function runs in parallel\n",
    "# wherever it can. TF function is usually faster\n",
    "# than the Python function.\n",
    "# By default, a TF function generates a new graph for every unique set of input_shapes,\n",
    "# and data types and caches it for subsequent calls. This is only true for tensor arguments.\n",
    "# If you call your TF function with a numerical Python value, a new graph will be generated\n",
    "# for every distinct value. Doing so for multiple values will take up a lot of RAM,\n",
    "# and slow down your computations. You have to delete the TF function to release the RAM used.\n",
    "# A better approach is to use Python values for arguments with few unique values ex.\n",
    "# hyperparameters like the number of neurons per layer.\n",
    "\n",
    "# When you write a custom loss function, custom metric, custom layer,\n",
    "# or any other custom function, and you use it in a Keras model,\n",
    "# Keras automatically converts your function into a TF function.\n",
    "\n",
    "# To tell Keras NOT to convert your Python functions to TF functions,\n",
    "# set dynamic=True when creating a custom layer or model.\n",
    "# Alternatively, when calling the model's compile(), set\n",
    "# run_eagerly=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is more common to decorate using tf.function\n",
    "@tf.function\n",
    "def cube(x):\n",
    "    return x ** 3\n",
    "\n",
    "# The original function is still available via the TF function's\n",
    "# python_function attribute\n",
    "tf_cube.python_function(2)  # Returns a scalar value, not a tensor, since this is a \n",
    "                            # python function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoGraph and Tracing\n",
    "\n",
    "How does TensorFlow generate Graphs?\n",
    "  - It analyzes the Python function's source code to capture all control flow statements\n",
    "  - It outputs an upgraded version of the function in which all the control flow statements\n",
    "    are replaced by the appropriate TF operations, ex. tf.while_loop() for loops, and \n",
    "    tf.cond() for if statements\n",
    "  - TF calls this upgraded function, but instead of passing the argument, it passes\n",
    "    the symbolic tensor - a tensor without any actual value, only a name, data type, \n",
    "    and shape.\n",
    "  - The function will run in graph mode (each TF operation will add a node in the graph\n",
    "    to represent itself and its output tensors). This is as opposed to eager execution\n",
    "    or eager mode. In graph mode TF operations do not perform any computations.\n",
    "  - For debugging purposes, you can view the generated function's source code by calling\n",
    "    tf.autograph.to_code(tf_cube.python_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tf__lambda = lambda x: ag__.with_function_scope(lambda lambda_scope: x ** 3, 'lambda_scope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True))\\n\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.autograph.to_code(lambda x: x ** 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def tf__test(x):\\n  do_return = False\\n  retval_ = ag__.UndefinedReturnValue()\\n  with ag__.FunctionScope('test', 'test_scope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as test_scope:\\n    do_return = True\\n    retval_ = test_scope.mark_return_value((x + 2) ** 3)\\n  do_return,\\n  return ag__.retval(retval_)\\n\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(x):\n",
    "    return (x + 2) ** 3\n",
    "tf.autograph.to_code(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Function Rules\n",
    "\n",
    "  - If you call any external library, it will not be part of the graph.\n",
    "    A TF graph can only include TF constructs (tensors, operations, variables, \n",
    "    datasets, etc.). This has a few additional implications:\n",
    "    - If your function just returns a random number using np.random.rand(), \n",
    "      a random number is created during tracing (the stage of building the graph).\n",
    "      This is why calls to the function using the same data type will return\n",
    "      the same random number. Instead, use tf.random.uniform() to generate a\n",
    "      random number on each call.\n",
    "    - Side-effects occur only during tracing (graph building)\n",
    "    - Wrapping arbitrary Python code in a tf.py_function() operation will hinder performance,\n",
    "      as TF cannot do any graph optimizations on this code. It will also reduce portability,\n",
    "      since the code will only run on systems where Python is available and the correct\n",
    "      libraries are installed.\n",
    "  - You can call other Python or TF functions, but they should follow the same rules.\n",
    "    These other functions do not need to be decorated with @tf.function\n",
    "  - If the function creates a TF variable (or a stateful TF object), it must do so upon\n",
    "    the very first call, and only then, or else you will get an exception.\n",
    "    It's preferable to create variables outside of the TF function\n",
    "    (ex. a custom layer's build() method).\n",
    "    To assign a new value to a variable, use v.assign() rathen than the = operator.\n",
    "  - Python function's source code should be available to TF. ex:\n",
    "    Defining your python code in a python shell, or\n",
    "    deploying only the compiled *.pyc files to production\n",
    "    will cause failures.\n",
    "  - TF will only capture for loops that iterate over a tensor or dataset, so use\n",
    "    for i in tf.range(x): rather than\n",
    "    for i in range(x):\n",
    "    or else the loop will not be captured in the graph. Instead it will run during tracing.\n",
    "    This may be what you want (ex. use the loop to create each layer in the network).\n",
    "  - Prefer a vectorized implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a custom layer that performs Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedLayer(keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, batch_input_size):\n",
    "        self.alpha = tf.Variable(1., shape=batch_input_size[-1:], data_type=tf.float32)\n",
    "        self.beta = tf.Variable(0., shape=batch_input_size[-1:], data_type=tf.float32)\n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    def call(self, X):\n",
    "        mean, var = tf.nn.moments(X, axes=[0])\n",
    "        std = tf.math.sqrt(var)\n",
    "        eps = 0.001\n",
    "        norm = self.alpha * (X - mean) / (std + eps) + self.beta\n",
    "        return self.activation(X @ self.kernel + self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
