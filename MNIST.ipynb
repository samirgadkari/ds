{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(22)\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = mnist_data  # load_data returns two tuples for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape, y_train_full.shape                       # shape of the training tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape                                   # shape of the testing tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('uint8'), dtype('uint8'))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype, y_train_full.dtype                       # dtypes in the training tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_flattened: min: 0, max: 255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0,   3,  18, 126, 136, 175,  26, 166, 255, 247, 127,  30,  36,\n",
       "        94, 154, 170, 253, 225, 172, 242, 195,  64,  49, 238, 251,  93,\n",
       "        82,  56,  39, 219, 198, 182, 241,  80, 156, 107, 205,  11,  43,\n",
       "        14,   1,  90, 139, 190,   2,  70,  35, 160, 108,  81, 240, 119,\n",
       "        25,  45, 186, 150,  27,  16, 252, 187, 249,  46, 130, 183, 207,\n",
       "       148, 229, 250,  24, 114, 221, 201,  78,  23,  66, 213, 171,   9,\n",
       "        55, 226, 244, 133, 212, 135, 132,  51, 159,  50,  48, 237,  54,\n",
       "       227, 239, 233,  57,   6,  10,  60, 224, 202,  84, 122, 163,  96,\n",
       "       189, 167, 228,  47,  79, 168, 179,  12,  75, 121,  21, 243,  38,\n",
       "       165, 208,   7, 178,  71,  19,  28,  63, 196,  76, 246, 112,  85,\n",
       "       230, 223, 131, 145, 173,  86, 162, 146,  29, 215, 199, 128, 141,\n",
       "        37,  67, 232,  62, 120, 180, 153, 210,  40, 220, 254, 222, 125,\n",
       "       245, 231, 216, 248,  91, 116, 144, 234, 143,   5, 177,  98, 102,\n",
       "       169, 137, 124,  68, 236, 211,  31,   8, 155,  20, 235,  32, 104,\n",
       "       184,  15, 193, 151,  89, 140, 113,  87,   4,  65,  92, 176, 129,\n",
       "        22, 197,  77, 217, 138,  42, 192, 109, 106, 218, 147,  13, 100,\n",
       "        33, 152, 181,  58, 157, 110, 194,  53, 117,  59, 203,  17, 200,\n",
       "        72, 105,  88,  73, 191, 149, 174, 158, 204,  74, 123,  52,  44,\n",
       "        61, 134, 188,  83, 103, 214, 209, 185, 206, 111, 118,  34,  41,\n",
       "       164,  69,  99, 142, 101, 115,  95, 161,  97], dtype=uint64)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_flattened = X_train_full.flatten()\n",
    "X_train_flattened_unique = pd.Series(X_train_flattened).unique()\n",
    "print(f'X_train_flattened: min: {min(X_train_flattened_unique)}, max: {max(X_train_flattened_unique)}')\n",
    "X_train_flattened_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.808798\n",
       "253    0.032169\n",
       "254    0.017040\n",
       "252    0.013901\n",
       "255    0.006681\n",
       "         ...   \n",
       "74     0.000294\n",
       "124    0.000293\n",
       "77     0.000288\n",
       "90     0.000270\n",
       "87     0.000269\n",
       "Length: 256, dtype: float64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(X_train_flattened).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = X_train_full.astype('float32')/255.  # X-values should be floats.\n",
    "                                                    # y-values can be integers because they're labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(X_train_full), np.max(X_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]  # Split full training set into validation and\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]  # training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 3, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(rows, cols):\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 10))\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            offset = row * rows + col\n",
    "            axes[row, col].imshow(X_train[offset], cmap='gray')\n",
    "            # axes[row, col].text(0, -2, class_names[y_train[offset]], c='r', size=24)\n",
    "            axes[row, col].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_images(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try a basic Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x139525ed0>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=[28, 28]),           # The shape of the weight matrix depends on the inputs.\n",
    "                                             # This is why we have to specify the input_shape, so future layers\n",
    "                                             # can know what size the weight matrix will be.\n",
    "    Dense(784, activation='relu'),           # You can set weights for a layer using kernel_initializer=...\n",
    "                                             # or using model.layer[1].set_weights(weights).\n",
    "    Dense(784, activation='relu'),           # You can set biases for a layer using bias_initializer=...\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_39 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 1,238,730\n",
      "Trainable params: 1,238,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()                                            # Show the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('flatten_39', 'dense_123', 'dense_124', 'dense_125')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer = model.layers[0]                              # Access model layers\n",
    "hidden1 = model.layers[1]\n",
    "hidden2 = model.layers[2]\n",
    "output = model.layers[3]\n",
    "input_layer.name, hidden1.name, hidden2.name, output.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 784), (784,))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, biases = hidden1.get_weights()                    # Get the weights and biases for a layer\n",
    "weights.shape, biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0597727 , -0.04125223,  0.04836189, ..., -0.05969794,\n",
       "        -0.03783982,  0.04883889],\n",
       "       [-0.01319176, -0.05230914, -0.00283706, ...,  0.01766376,\n",
       "         0.04966957, -0.02501345],\n",
       "       [ 0.01293183,  0.06077526,  0.01215701, ...,  0.05641463,\n",
       "         0.012023  ,  0.02866104],\n",
       "       ...,\n",
       "       [ 0.05329112, -0.04961664, -0.06068813, ..., -0.06097025,\n",
       "         0.01778476,  0.0436982 ],\n",
       "       [ 0.00359493, -0.00354032, -0.05303804, ...,  0.0447243 ,\n",
       "        -0.05217056, -0.02766738],\n",
       "       [-0.05474851, -0.02831792, -0.04555859, ...,  0.01482938,\n",
       "        -0.0033611 ,  0.01055476]], dtype=float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights                                                    # Weights are randomly initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases                                                     # Biases are initialized to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_logdir: ./my_logs\n"
     ]
    }
   ],
   "source": [
    "root_logdir = os.path.join(os.curdir, 'my_logs')\n",
    "print(f'root_logdir: {root_logdir}')\n",
    "\n",
    "def get_run_logdir(lr):\n",
    "    import time\n",
    "    run_id = time.strftime(f'run_%Y_%m_%d_%H_%M_%S_{lr}')\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "                    os.path.join(os.curdir, 'data', \n",
    "                                 'best_model_while_searching_lr.h5'),\n",
    "                    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "                        patience=10,\n",
    "                        restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_with_different_lr(lr=0.01):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=[28, 28]),           # The shape of the weight matrix depends on the inputs.\n",
    "                                                 # This is why we have to specify the input_shape, so future layers\n",
    "                                                 # can know what size the weight matrix will be.\n",
    "        Dense(784, activation='relu'),           # You can set weights for a layer using kernel_initializer=...\n",
    "                                                 # or using model.layer[1].set_weights(weights).\n",
    "        Dense(784, activation='relu'),           # You can set biases for a layer using bias_initializer=...\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',  # 'sparse_categorical_crossentropy' is same as\n",
    "                                                           # keras.losses.sparse_categorical_crossentropy\n",
    "                                                           # We're using this loss since we have sparse labels (\n",
    "                                                           # on value per instance, rather than one-hot-encoded lables).\n",
    "                                                           #\n",
    "                                                           # Here's what to use based on label types:\n",
    "                                                           #   label type     loss function              activation_function\n",
    "                                                           #\n",
    "                                                           # one-hot-labels  categorical_crossentropy         sigmoid\n",
    "                                                           # sparse labels   sparse_categorical_crossentropy  softmax\n",
    "                                                           # \n",
    "                                                           # sparse labels > keras.utils.to_categorical() > one-hot-labels\n",
    "                                                           # sparse labels <         np.argmax()          < one-hot-labels\n",
    "                                                           #\n",
    "                  optimizer=keras.optimizers.SGD(lr=1), # 'sgd' is same as keras.optimizers.SGD(), \n",
    "                                                           # except default values are used here.\n",
    "                                                           # With 'sgd', learning_rate defaults to 0.01.\n",
    "                  metrics=['accuracy']) # Any extra metrics we want to see.\n",
    "                                        # 'accuracy' is same as keras.metrics.sparse_categorical_accuracy\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_logdir: ./my_logs/run_2019_10_28_20_23_40_0.001\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 10s 182us/sample - loss: 1.0176 - accuracy: 0.6812 - val_loss: 0.3148 - val_accuracy: 0.9156\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 9s 171us/sample - loss: 0.3346 - accuracy: 0.9123 - val_loss: 0.2299 - val_accuracy: 0.9384\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 9s 172us/sample - loss: 0.2655 - accuracy: 0.9325 - val_loss: 0.2599 - val_accuracy: 0.9410\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 9s 172us/sample - loss: 0.2452 - accuracy: 0.9404 - val_loss: 0.2983 - val_accuracy: 0.9326\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 9s 173us/sample - loss: 0.2133 - accuracy: 0.9477 - val_loss: 0.3360 - val_accuracy: 0.9076\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 10s 173us/sample - loss: 0.2168 - accuracy: 0.9471 - val_loss: 0.2140 - val_accuracy: 0.9522\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 9s 171us/sample - loss: 0.2214 - accuracy: 0.9481 - val_loss: 0.4404 - val_accuracy: 0.9258\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 10s 173us/sample - loss: 0.2140 - accuracy: 0.9513 - val_loss: 0.3208 - val_accuracy: 0.9472\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 10s 174us/sample - loss: 0.1993 - accuracy: 0.9543 - val_loss: 0.2077 - val_accuracy: 0.9570\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 10s 174us/sample - loss: 0.2273 - accuracy: 0.9521 - val_loss: 0.3398 - val_accuracy: 0.9352\n",
      "Epoch 11/100\n",
      "55000/55000 [==============================] - 10s 174us/sample - loss: 0.1896 - accuracy: 0.9564 - val_loss: 0.2548 - val_accuracy: 0.9556\n",
      "Epoch 12/100\n",
      "55000/55000 [==============================] - 10s 174us/sample - loss: 0.1520 - accuracy: 0.9638 - val_loss: 0.2255 - val_accuracy: 0.9590\n",
      "Epoch 13/100\n",
      "55000/55000 [==============================] - 10s 173us/sample - loss: nan - accuracy: 0.6891 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 14/100\n",
      "55000/55000 [==============================] - 10s 174us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 15/100\n",
      "55000/55000 [==============================] - 10s 175us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 16/100\n",
      "55000/55000 [==============================] - 10s 174us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 17/100\n",
      "55000/55000 [==============================] - 10s 175us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 18/100\n",
      "55000/55000 [==============================] - 10s 176us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 19/100\n",
      "55000/55000 [==============================] - 10s 175us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "run_logdir: ./my_logs/run_2019_10_28_20_26_42_0.01\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 10s 184us/sample - loss: 0.8268 - accuracy: 0.7592 - val_loss: 0.2816 - val_accuracy: 0.9250\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 10s 178us/sample - loss: 0.3100 - accuracy: 0.9202 - val_loss: 0.2047 - val_accuracy: 0.9488\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 10s 174us/sample - loss: 0.2443 - accuracy: 0.9376 - val_loss: 0.2653 - val_accuracy: 0.9448\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 10s 177us/sample - loss: 0.2205 - accuracy: 0.9455 - val_loss: 0.1993 - val_accuracy: 0.9528\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 10s 179us/sample - loss: 0.2094 - accuracy: 0.9491 - val_loss: 0.1601 - val_accuracy: 0.9628\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 10s 177us/sample - loss: 0.1802 - accuracy: 0.9554 - val_loss: 0.2919 - val_accuracy: 0.9376\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 10s 181us/sample - loss: 0.1825 - accuracy: 0.9560 - val_loss: 0.3138 - val_accuracy: 0.9526\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 10s 178us/sample - loss: 0.1652 - accuracy: 0.9614 - val_loss: 0.1709 - val_accuracy: 0.9612\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 10s 179us/sample - loss: 0.2115 - accuracy: 0.9556 - val_loss: 0.1955 - val_accuracy: 0.9586\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 10s 177us/sample - loss: 0.1549 - accuracy: 0.9627 - val_loss: 0.1834 - val_accuracy: 0.9606\n",
      "Epoch 11/100\n",
      "55000/55000 [==============================] - 10s 180us/sample - loss: 0.1602 - accuracy: 0.9635 - val_loss: 0.2004 - val_accuracy: 0.9642\n",
      "Epoch 12/100\n",
      "55000/55000 [==============================] - 10s 179us/sample - loss: 0.1536 - accuracy: 0.9650 - val_loss: 0.1934 - val_accuracy: 0.9642\n",
      "Epoch 13/100\n",
      "55000/55000 [==============================] - 10s 177us/sample - loss: 0.1678 - accuracy: 0.9647 - val_loss: 0.2234 - val_accuracy: 0.9596\n",
      "Epoch 14/100\n",
      "55000/55000 [==============================] - 10s 181us/sample - loss: 0.1567 - accuracy: 0.9653 - val_loss: 0.2489 - val_accuracy: 0.9596\n",
      "Epoch 15/100\n",
      "55000/55000 [==============================] - 10s 180us/sample - loss: 0.1407 - accuracy: 0.9691 - val_loss: 0.3150 - val_accuracy: 0.9378\n",
      "run_logdir: ./my_logs/run_2019_10_28_20_29_10_0.1\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 10s 187us/sample - loss: 1.2677 - accuracy: 0.5965 - val_loss: 0.5788 - val_accuracy: 0.8254\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 9s 171us/sample - loss: 0.3986 - accuracy: 0.8974 - val_loss: 0.2588 - val_accuracy: 0.9352\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 9s 171us/sample - loss: 0.2878 - accuracy: 0.9274 - val_loss: 0.2786 - val_accuracy: 0.9366\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 9s 170us/sample - loss: 0.2329 - accuracy: 0.9418 - val_loss: 0.2158 - val_accuracy: 0.9464\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 10s 173us/sample - loss: 0.2004 - accuracy: 0.9500 - val_loss: 0.4861 - val_accuracy: 0.9216\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 10s 173us/sample - loss: 0.2631 - accuracy: 0.9408 - val_loss: 0.3163 - val_accuracy: 0.9264\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 10s 173us/sample - loss: 0.2187 - accuracy: 0.9483 - val_loss: 0.2104 - val_accuracy: 0.9524\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 9s 172us/sample - loss: 0.1992 - accuracy: 0.9539 - val_loss: 0.2495 - val_accuracy: 0.9520\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 10s 173us/sample - loss: 0.1878 - accuracy: 0.9561 - val_loss: 0.2361 - val_accuracy: 0.9448\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 9s 169us/sample - loss: 0.1764 - accuracy: 0.9587 - val_loss: 0.2493 - val_accuracy: 0.9588\n",
      "Epoch 11/100\n",
      "55000/55000 [==============================] - 10s 173us/sample - loss: 0.1551 - accuracy: 0.9631 - val_loss: 0.2449 - val_accuracy: 0.9588\n",
      "Epoch 12/100\n",
      "55000/55000 [==============================] - 10s 173us/sample - loss: 0.1661 - accuracy: 0.9622 - val_loss: 0.3456 - val_accuracy: 0.9476\n",
      "Epoch 13/100\n",
      "55000/55000 [==============================] - 9s 170us/sample - loss: 0.1601 - accuracy: 0.9633 - val_loss: 0.2809 - val_accuracy: 0.9412\n",
      "Epoch 14/100\n",
      "55000/55000 [==============================] - 10s 173us/sample - loss: 0.1694 - accuracy: 0.9611 - val_loss: 0.2466 - val_accuracy: 0.9574\n",
      "Epoch 15/100\n",
      "55000/55000 [==============================] - 9s 173us/sample - loss: 0.1573 - accuracy: 0.9642 - val_loss: 0.2092 - val_accuracy: 0.9638\n",
      "Epoch 16/100\n",
      "55000/55000 [==============================] - 10s 174us/sample - loss: 0.1505 - accuracy: 0.9661 - val_loss: 0.2118 - val_accuracy: 0.9614\n",
      "Epoch 17/100\n",
      "55000/55000 [==============================] - 10s 173us/sample - loss: 0.1554 - accuracy: 0.9657 - val_loss: 0.2187 - val_accuracy: 0.9604\n",
      "Epoch 18/100\n",
      "55000/55000 [==============================] - 10s 174us/sample - loss: 0.2167 - accuracy: 0.9572 - val_loss: 0.2692 - val_accuracy: 0.9566\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000/55000 [==============================] - 9s 169us/sample - loss: 0.1659 - accuracy: 0.9637 - val_loss: 0.2937 - val_accuracy: 0.9598\n",
      "Epoch 20/100\n",
      "55000/55000 [==============================] - 9s 169us/sample - loss: 0.1495 - accuracy: 0.9668 - val_loss: 0.2127 - val_accuracy: 0.9656\n",
      "Epoch 21/100\n",
      "55000/55000 [==============================] - 9s 167us/sample - loss: 0.1273 - accuracy: 0.9710 - val_loss: 0.2378 - val_accuracy: 0.9610\n",
      "Epoch 22/100\n",
      "55000/55000 [==============================] - 9s 169us/sample - loss: 0.1256 - accuracy: 0.9715 - val_loss: 0.2321 - val_accuracy: 0.9630\n",
      "Epoch 23/100\n",
      "55000/55000 [==============================] - 9s 166us/sample - loss: 0.1504 - accuracy: 0.9674 - val_loss: 0.2604 - val_accuracy: 0.9586\n",
      "Epoch 24/100\n",
      "55000/55000 [==============================] - 9s 171us/sample - loss: 0.1532 - accuracy: 0.9667 - val_loss: 0.2743 - val_accuracy: 0.9584\n",
      "Epoch 25/100\n",
      "55000/55000 [==============================] - 9s 171us/sample - loss: 0.1445 - accuracy: 0.9697 - val_loss: 0.2492 - val_accuracy: 0.9576\n",
      "run_logdir: ./my_logs/run_2019_10_28_20_33_07_1.0\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 10s 175us/sample - loss: 1.6441 - accuracy: 0.4414 - val_loss: 0.9999 - val_accuracy: 0.6660\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 9s 169us/sample - loss: 1.1416 - accuracy: 0.6491 - val_loss: 0.8159 - val_accuracy: 0.7842\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 9s 168us/sample - loss: 0.5713 - accuracy: 0.8597 - val_loss: 0.3737 - val_accuracy: 0.9074\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 9s 167us/sample - loss: 0.5054 - accuracy: 0.8829 - val_loss: 0.6124 - val_accuracy: 0.7888\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 9s 171us/sample - loss: 0.3769 - accuracy: 0.9067 - val_loss: 0.4142 - val_accuracy: 0.9054\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 9s 172us/sample - loss: 0.3211 - accuracy: 0.9222 - val_loss: 0.2878 - val_accuracy: 0.9348\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 9s 169us/sample - loss: 0.3187 - accuracy: 0.9231 - val_loss: 0.3442 - val_accuracy: 0.9294\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 10s 181us/sample - loss: 0.3128 - accuracy: 0.9253 - val_loss: 0.2556 - val_accuracy: 0.9400\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 10s 184us/sample - loss: 0.2803 - accuracy: 0.9312 - val_loss: 0.2495 - val_accuracy: 0.9472\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 9s 167us/sample - loss: 0.2548 - accuracy: 0.9391 - val_loss: 0.3021 - val_accuracy: 0.9324\n",
      "Epoch 11/100\n",
      "55000/55000 [==============================] - 9s 167us/sample - loss: 0.3035 - accuracy: 0.9309 - val_loss: 0.2873 - val_accuracy: 0.9366\n",
      "Epoch 12/100\n",
      "55000/55000 [==============================] - 9s 170us/sample - loss: 0.3216 - accuracy: 0.9290 - val_loss: 0.3254 - val_accuracy: 0.9436\n",
      "Epoch 13/100\n",
      "55000/55000 [==============================] - 10s 174us/sample - loss: 0.2624 - accuracy: 0.9411 - val_loss: 0.3153 - val_accuracy: 0.9406\n",
      "Epoch 14/100\n",
      "55000/55000 [==============================] - 9s 170us/sample - loss: 0.2713 - accuracy: 0.9388 - val_loss: 0.2351 - val_accuracy: 0.9458\n",
      "Epoch 15/100\n",
      "55000/55000 [==============================] - 9s 171us/sample - loss: 0.2314 - accuracy: 0.9441 - val_loss: 0.2872 - val_accuracy: 0.9394\n",
      "Epoch 16/100\n",
      "55000/55000 [==============================] - 9s 171us/sample - loss: 0.2631 - accuracy: 0.9413 - val_loss: 0.2717 - val_accuracy: 0.9476\n",
      "Epoch 17/100\n",
      "55000/55000 [==============================] - 10s 175us/sample - loss: 0.2298 - accuracy: 0.9467 - val_loss: 0.3519 - val_accuracy: 0.9392\n",
      "Epoch 18/100\n",
      "55000/55000 [==============================] - 10s 174us/sample - loss: 0.2445 - accuracy: 0.9470 - val_loss: 0.2744 - val_accuracy: 0.9494\n",
      "Epoch 19/100\n",
      "55000/55000 [==============================] - 10s 187us/sample - loss: 0.2595 - accuracy: 0.9430 - val_loss: 0.3376 - val_accuracy: 0.9330\n",
      "Epoch 20/100\n",
      "55000/55000 [==============================] - 10s 189us/sample - loss: 0.2792 - accuracy: 0.9381 - val_loss: 0.3495 - val_accuracy: 0.9392\n",
      "Epoch 21/100\n",
      "55000/55000 [==============================] - 10s 185us/sample - loss: 0.2776 - accuracy: 0.9412 - val_loss: 0.2955 - val_accuracy: 0.9480\n",
      "Epoch 22/100\n",
      "55000/55000 [==============================] - 11s 194us/sample - loss: 0.2707 - accuracy: 0.9398 - val_loss: 0.3240 - val_accuracy: 0.9468\n",
      "Epoch 23/100\n",
      "55000/55000 [==============================] - 10s 173us/sample - loss: 0.2883 - accuracy: 0.9353 - val_loss: 0.3053 - val_accuracy: 0.9332\n",
      "Epoch 24/100\n",
      "55000/55000 [==============================] - 10s 173us/sample - loss: 0.2444 - accuracy: 0.9426 - val_loss: 0.2546 - val_accuracy: 0.9464\n"
     ]
    }
   ],
   "source": [
    "for lr in [0.001, 0.01, 0.1, 1.0]:\n",
    "    model = mnist_with_different_lr(lr)\n",
    "    run_logdir = get_run_logdir(lr)\n",
    "    print(f'run_logdir: {run_logdir}')\n",
    "    \n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "    history = model.fit(X_train, y_train, epochs=100,\n",
    "                        validation_data=(X_valid, y_valid),\n",
    "                        callbacks=[tensorboard_cb,\n",
    "                                   checkpoint_cb,\n",
    "                                   early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "# plt.grid(True)\n",
    "# plt.gca().set_ylim([0.0, 1.0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_logdir: ./my_logs/run_2019_10_28_20_36_58_0.005\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 10s 178us/sample - loss: nan - accuracy: 0.1025 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 9s 166us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 9s 168us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 9s 170us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 9s 170us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 9s 169us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 9s 169us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 9s 170us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 9s 170us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 9s 170us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "run_logdir: ./my_logs/run_2019_10_28_20_38_32_0.015\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 10s 178us/sample - loss: 0.4607 - accuracy: 0.8658 - val_loss: 0.2466 - val_accuracy: 0.9364\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 9s 167us/sample - loss: 0.2062 - accuracy: 0.9439 - val_loss: 0.1506 - val_accuracy: 0.9596\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 9s 171us/sample - loss: 0.1809 - accuracy: 0.9538 - val_loss: 0.2710 - val_accuracy: 0.9348\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 9s 168us/sample - loss: 0.1664 - accuracy: 0.9574 - val_loss: 0.2121 - val_accuracy: 0.9554\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 9s 171us/sample - loss: 0.1536 - accuracy: 0.9623 - val_loss: 0.3160 - val_accuracy: 0.9310\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 9s 172us/sample - loss: 0.1398 - accuracy: 0.9658 - val_loss: 0.1723 - val_accuracy: 0.9636\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 9s 171us/sample - loss: 0.1254 - accuracy: 0.9690 - val_loss: 0.2345 - val_accuracy: 0.9574\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 9s 172us/sample - loss: 0.1284 - accuracy: 0.9703 - val_loss: 0.2116 - val_accuracy: 0.9588\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 9s 171us/sample - loss: 0.1384 - accuracy: 0.9697 - val_loss: 0.2317 - val_accuracy: 0.9670\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 9s 172us/sample - loss: 0.1371 - accuracy: 0.9695 - val_loss: 0.1924 - val_accuracy: 0.9640\n",
      "Epoch 11/100\n",
      "55000/55000 [==============================] - 9s 170us/sample - loss: 0.1491 - accuracy: 0.9694 - val_loss: 0.2219 - val_accuracy: 0.9600\n",
      "Epoch 12/100\n",
      "55000/55000 [==============================] - 9s 169us/sample - loss: nan - accuracy: 0.9411 - val_loss: nan - val_accuracy: 0.0958\n",
      "run_logdir: ./my_logs/run_2019_10_28_20_40_25_0.024999999999999998\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 10s 179us/sample - loss: nan - accuracy: 0.1020 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 9s 171us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 9s 168us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 9s 167us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 9s 169us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 9s 170us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 9s 171us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 9s 170us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 9s 169us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 9s 169us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "run_logdir: ./my_logs/run_2019_10_28_20_41_59_0.034999999999999996\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 10s 180us/sample - loss: 0.7801 - accuracy: 0.7723 - val_loss: 0.4560 - val_accuracy: 0.8600\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 9s 172us/sample - loss: 0.2709 - accuracy: 0.9309 - val_loss: 0.2882 - val_accuracy: 0.9258\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 9s 173us/sample - loss: 0.2196 - accuracy: 0.9448 - val_loss: 0.2041 - val_accuracy: 0.9526\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 9s 172us/sample - loss: 0.1774 - accuracy: 0.9554 - val_loss: 0.2089 - val_accuracy: 0.9532\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 9s 170us/sample - loss: nan - accuracy: 0.3074 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 9s 172us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 10s 174us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 9s 172us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 10s 174us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 9s 172us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 11/100\n",
      "55000/55000 [==============================] - 9s 171us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 12/100\n",
      "55000/55000 [==============================] - 9s 172us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 13/100\n",
      "55000/55000 [==============================] - 10s 173us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "run_logdir: ./my_logs/run_2019_10_28_20_44_03_0.04499999999999999\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 10s 191us/sample - loss: 1.0756 - accuracy: 0.6829 - val_loss: 0.3749 - val_accuracy: 0.8970\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 10s 179us/sample - loss: 0.3531 - accuracy: 0.9073 - val_loss: 0.2395 - val_accuracy: 0.9388\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 10s 176us/sample - loss: 0.2607 - accuracy: 0.9324 - val_loss: 0.4698 - val_accuracy: 0.9060\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 10s 173us/sample - loss: 0.3017 - accuracy: 0.9288 - val_loss: 0.2300 - val_accuracy: 0.9430\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 10s 174us/sample - loss: 0.2092 - accuracy: 0.9475 - val_loss: 0.3085 - val_accuracy: 0.9188\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 10s 177us/sample - loss: nan - accuracy: 0.6778 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 10s 177us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 10s 178us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 10s 178us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 10s 176us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 11/100\n",
      "55000/55000 [==============================] - 10s 177us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 12/100\n",
      "55000/55000 [==============================] - 10s 180us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 13/100\n",
      "55000/55000 [==============================] - 10s 180us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 14/100\n",
      "55000/55000 [==============================] - 10s 177us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n"
     ]
    }
   ],
   "source": [
    "for lr in np.arange(0.005, 0.05, 0.01):\n",
    "    model = mnist_with_different_lr(lr)\n",
    "    run_logdir = get_run_logdir(lr)\n",
    "    print(f'run_logdir: {run_logdir}')\n",
    "    \n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "    history = model.fit(X_train, y_train, epochs=100,\n",
    "                        validation_data=(X_valid, y_valid),\n",
    "                        callbacks=[tensorboard_cb,\n",
    "                                   checkpoint_cb,\n",
    "                                   early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_logdir: ./my_logs/run_2019_10_28_20_46_20_0.02\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 10s 184us/sample - loss: 0.7264 - accuracy: 0.7831 - val_loss: 0.2566 - val_accuracy: 0.9330\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 10s 177us/sample - loss: 0.2737 - accuracy: 0.9270 - val_loss: 0.3179 - val_accuracy: 0.9152\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 10s 178us/sample - loss: 0.2186 - accuracy: 0.9437 - val_loss: 0.1877 - val_accuracy: 0.9492\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 10s 176us/sample - loss: 0.1896 - accuracy: 0.9521 - val_loss: 0.2041 - val_accuracy: 0.9560\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 10s 182us/sample - loss: 0.1740 - accuracy: 0.9563 - val_loss: 0.1872 - val_accuracy: 0.9588\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 10s 179us/sample - loss: 0.1679 - accuracy: 0.9595 - val_loss: 0.1757 - val_accuracy: 0.9596\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 10s 179us/sample - loss: 0.1551 - accuracy: 0.9628 - val_loss: 0.1704 - val_accuracy: 0.9660\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 10s 179us/sample - loss: 0.1493 - accuracy: 0.9646 - val_loss: 0.7062 - val_accuracy: 0.9324\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 10s 181us/sample - loss: nan - accuracy: 0.2827 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 10s 188us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 11/100\n",
      "55000/55000 [==============================] - 9s 166us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 12/100\n",
      "55000/55000 [==============================] - 9s 162us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 13/100\n",
      "55000/55000 [==============================] - 9s 170us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 14/100\n",
      "55000/55000 [==============================] - 9s 168us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 15/100\n",
      "55000/55000 [==============================] - 9s 166us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 16/100\n",
      "55000/55000 [==============================] - 9s 164us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 17/100\n",
      "55000/55000 [==============================] - 9s 162us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "run_logdir: ./my_logs/run_2019_10_28_20_49_03_0.023\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 10s 181us/sample - loss: nan - accuracy: 0.1081 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 9s 168us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 9s 164us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 9s 165us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 9s 172us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 9s 166us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 9s 170us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 10s 176us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 9s 170us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 9s 166us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "run_logdir: ./my_logs/run_2019_10_28_20_50_37_0.026\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 10s 182us/sample - loss: nan - accuracy: 0.1021 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 9s 168us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 9s 162us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 9s 168us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 9s 168us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 9s 166us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 9s 168us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 9s 169us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 9s 165us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 9s 167us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "run_logdir: ./my_logs/run_2019_10_28_20_52_10_0.028999999999999998\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 10s 180us/sample - loss: nan - accuracy: 0.1061 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 9s 171us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 9s 166us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 9s 171us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 9s 170us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 9s 170us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 9s 165us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 9s 164us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 10s 177us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 11s 198us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "run_logdir: ./my_logs/run_2019_10_28_20_53_46_0.032\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 10s 183us/sample - loss: nan - accuracy: 0.1007 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 9s 164us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 9s 172us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 10s 174us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 9s 165us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 9s 168us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 9s 169us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 9s 165us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 9s 165us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 9s 171us/sample - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n"
     ]
    }
   ],
   "source": [
    "for lr in np.arange(0.02, 0.033, 0.003):\n",
    "    model = mnist_with_different_lr(lr)\n",
    "    run_logdir = get_run_logdir(lr)\n",
    "    print(f'run_logdir: {run_logdir}')\n",
    "    \n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "    history = model.fit(X_train, y_train, epochs=100,\n",
    "                        validation_data=(X_valid, y_valid),\n",
    "                        callbacks=[tensorboard_cb,\n",
    "                                   checkpoint_cb,\n",
    "                                   early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MNIST Tensorboard plots](img/MNIST_tensorboard_plots.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
